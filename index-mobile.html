<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>EVA - Mobile Chat</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 64 64'%3E%3Crect width='64' height='64' rx='12' fill='%23007BFF'/%3E%3Ctext x='50%25' y='54%25' font-size='34' text-anchor='middle' fill='white' font-family='Segoe UI, Roboto, sans-serif'%3EE%3C/text%3E%3C/svg%3E">
    
    <style>
        /* Reset and Base Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html, body {
            height: 100%;
            overflow: hidden;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background-color: #000000;
            color: #ffffff;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            touch-action: manipulation;
        }

        /* Prevent text selection on buttons */
        button, .icon-btn {
            -webkit-user-select: none;
            -moz-user-select: none;
            user-select: none;
            -webkit-tap-highlight-color: transparent;
        }

        /* Header Bar */
        .header-bar {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: 56px;
            background-color: #000000;
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 0 16px;
            padding-top: env(safe-area-inset-top);
            z-index: 1000;
            border-bottom: 1px solid #1a1a1a;
        }

        .header-left, .header-right {
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .icon-btn {
            width: 44px;
            height: 44px;
            min-width: 44px;
            min-height: 44px;
            background: transparent;
            border: none;
            color: #ffffff;
            font-size: 20px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 8px;
            transition: background-color 0.2s;
        }

        .icon-btn:active {
            background-color: #1a1a1a;
        }

        .app-title {
            font-size: 18px;
            font-weight: 600;
            color: #ffffff;
            flex: 1;
            text-align: center;
        }

        /* Main Content Area */
        .main-content {
            position: fixed;
            top: 56px;
            top: calc(56px + env(safe-area-inset-top));
            left: 0;
            right: 0;
            bottom: 80px;
            bottom: calc(80px + env(safe-area-inset-bottom));
            display: flex;
            flex-direction: column;
            overflow: hidden;
            background-color: #000000;
        }

        /* Avatar Container */
        .avatar-container {
            flex: 0 0 auto;
            width: 100%;
            max-width: 100%;
            height: 40vh;
            min-height: 300px;
            max-height: 400px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            background-color: #000000;
        }

        #live2d-container, #vrm-container {
            width: 100%;
            height: 100%;
            position: relative;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        #vrm-container {
            display: none;
        }

        #live2d-canvas, #vrm-canvas {
            width: 100%;
            height: 100%;
            max-width: 100%;
            max-height: 100%;
        }

        /* Message History Area */
        .message-history-container {
            flex: 1 1 auto;
            overflow-y: auto;
            overflow-x: hidden;
            padding: 16px;
            -webkit-overflow-scrolling: touch;
            scroll-behavior: smooth;
        }

        .message {
            margin-bottom: 16px;
            padding: 12px 16px;
            border-radius: 12px;
            line-height: 1.5;
            word-wrap: break-word;
            max-width: 85%;
        }

        .message.user {
            background-color: #1e3a5f;
            border-left: 4px solid #4a9eff;
            margin-left: auto;
            margin-right: 0;
        }

        .message.assistant {
            background-color: #1a3a1a;
            border-left: 4px solid #4ade80;
            margin-left: 0;
            margin-right: auto;
        }

        .message.philosopher {
            background-color: #3a2a1a;
            border-left: 4px solid #ff9800;
            margin-left: 0;
            margin-right: auto;
        }

        .message-sender {
            font-weight: 600;
            margin-bottom: 6px;
            font-size: 14px;
            opacity: 0.9;
        }

        .message.user .message-sender {
            color: #4a9eff;
        }

        .message.assistant .message-sender {
            color: #4ade80;
        }

        .message.philosopher .message-sender {
            color: #ff9800;
        }

        .message-content {
            color: #e0e0e0;
            white-space: pre-wrap;
            font-size: 16px;
            line-height: 1.5;
        }

        /* Input Bar */
        .input-bar {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background-color: #1a1a1a;
            border-top: 1px solid #2a2a2a;
            padding: 12px 16px;
            padding-bottom: calc(12px + env(safe-area-inset-bottom));
            display: flex;
            align-items: center;
            gap: 12px;
            z-index: 1000;
        }

        .input-wrapper {
            flex: 1;
            display: flex;
            align-items: center;
            background-color: #2a2a2a;
            border-radius: 24px;
            padding: 8px 16px;
            min-height: 48px;
        }

        #user-input {
            flex: 1;
            background: transparent;
            border: none;
            color: #ffffff;
            font-size: 16px;
            font-family: inherit;
            outline: none;
            resize: none;
            max-height: 120px;
            overflow-y: auto;
            padding: 0;
            line-height: 1.5;
        }

        #user-input::placeholder {
            color: #999999;
        }

        .input-actions {
            display: flex;
            gap: 8px;
            align-items: center;
        }

        .action-btn {
            width: 44px;
            height: 44px;
            min-width: 44px;
            min-height: 44px;
            background-color: #007BFF;
            border: none;
            border-radius: 50%;
            color: #ffffff;
            font-size: 18px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background-color 0.2s, transform 0.1s;
            flex-shrink: 0;
        }

        .action-btn:active {
            transform: scale(0.95);
            background-color: #0056b3;
        }

        .action-btn.recording {
            background-color: #dc3545;
            animation: pulse 1.5s ease-in-out infinite;
        }

        .action-btn.philosopher-active {
            background-color: #ff9800;
        }

        @keyframes pulse {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: 0.7;
            }
        }

        /* Drawers */
        .drawer-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 2000;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease-in-out, visibility 0.3s ease-in-out;
        }

        .drawer-overlay.active {
            opacity: 1;
            visibility: visible;
        }

        .drawer {
            position: fixed;
            top: 0;
            bottom: 0;
            width: 85%;
            max-width: 320px;
            background-color: #1a1a1a;
            z-index: 2001;
            overflow-y: auto;
            -webkit-overflow-scrolling: touch;
            box-shadow: 2px 0 10px rgba(0, 0, 0, 0.3);
        }

        .drawer.left {
            left: 0;
            transform: translateX(-100%);
            transition: transform 0.3s ease-in-out;
        }

        .drawer.left.active {
            transform: translateX(0);
        }

        .drawer.right {
            right: 0;
            transform: translateX(100%);
            transition: transform 0.3s ease-in-out;
        }

        .drawer.right.active {
            transform: translateX(0);
        }

        .drawer-header {
            padding: 16px;
            padding-top: calc(16px + env(safe-area-inset-top));
            border-bottom: 1px solid #2a2a2a;
            display: flex;
            align-items: center;
            justify-content: space-between;
            background-color: #000000;
            position: sticky;
            top: 0;
            z-index: 10;
        }

        .drawer-title {
            font-size: 20px;
            font-weight: 600;
            color: #ffffff;
        }

        .drawer-close {
            width: 36px;
            height: 36px;
            background: transparent;
            border: none;
            color: #ffffff;
            font-size: 20px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 8px;
        }

        .drawer-close:active {
            background-color: #2a2a2a;
        }

        .drawer-content {
            padding: 16px;
            padding-bottom: 32px;
            min-height: calc(100% - 60px);
        }

        /* Chat History Drawer */
        .new-chat-btn {
            width: 100%;
            padding: 12px;
            background-color: #28a745;
            border: none;
            border-radius: 8px;
            color: #ffffff;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            margin-bottom: 16px;
            transition: background-color 0.2s;
        }

        .new-chat-btn:active {
            background-color: #218838;
        }

        .conversation-list {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }

        .conversation-item {
            padding: 12px;
            background-color: #2a2a2a;
            border-radius: 8px;
            cursor: pointer;
            transition: background-color 0.2s;
        }

        .conversation-item:active {
            background-color: #3a3a3a;
        }

        .conversation-item.active {
            background-color: #1e3a5f;
            border: 1px solid #4a9eff;
        }

        .conversation-title {
            font-size: 16px;
            color: #ffffff;
            margin-bottom: 4px;
            word-wrap: break-word;
        }

        .conversation-date {
            font-size: 12px;
            color: #999999;
        }

        /* Settings Drawer */
        .settings-section {
            margin-bottom: 24px;
        }

        .settings-section-title {
            font-size: 18px;
            font-weight: 600;
            color: #ffffff;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid #2a2a2a;
        }

        .settings-group {
            margin-bottom: 16px;
        }

        .settings-label {
            display: block;
            font-size: 14px;
            color: #e0e0e0;
            margin-bottom: 8px;
        }

        .settings-input {
            width: 100%;
            padding: 12px;
            background-color: #2a2a2a;
            border: 1px solid #3a3a3a;
            border-radius: 8px;
            color: #ffffff;
            font-size: 16px;
            font-family: inherit;
            outline: none;
        }

        .settings-input:focus {
            border-color: #007BFF;
        }

        .settings-textarea {
            width: 100%;
            padding: 12px;
            background-color: #2a2a2a;
            border: 1px solid #3a3a3a;
            border-radius: 8px;
            color: #ffffff;
            font-size: 16px;
            font-family: inherit;
            outline: none;
            resize: vertical;
            min-height: 80px;
        }

        .settings-select {
            width: 100%;
            padding: 12px;
            background-color: #2a2a2a;
            border: 1px solid #3a3a3a;
            border-radius: 8px;
            color: #ffffff;
            font-size: 16px;
            font-family: inherit;
            outline: none;
        }

        .toggle-switch {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 12px 0;
        }

        .toggle-switch-label {
            font-size: 16px;
            color: #e0e0e0;
        }

        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 28px;
        }

        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #3a3a3a;
            transition: 0.3s;
            border-radius: 28px;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 22px;
            width: 22px;
            left: 3px;
            bottom: 3px;
            background-color: #ffffff;
            transition: 0.3s;
            border-radius: 50%;
        }

        input:checked + .slider {
            background-color: #007BFF;
        }

        input:checked + .slider:before {
            transform: translateX(22px);
        }

        .helper-text {
            font-size: 12px;
            color: #999999;
            margin-top: 4px;
            display: block;
        }

        /* Status indicator */
        .status-indicator {
            position: fixed;
            bottom: calc(80px + env(safe-area-inset-bottom));
            left: 50%;
            transform: translateX(-50%);
            background-color: rgba(0, 0, 0, 0.8);
            color: #ffffff;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            z-index: 999;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s, visibility 0.3s;
        }

        .status-indicator.active {
            opacity: 1;
            visibility: visible;
        }

        /* Loading state */
        .message-history-container.responding {
            position: relative;
        }

        .message-history-container.responding::after {
            content: '';
            position: absolute;
            bottom: 16px;
            left: 50%;
            transform: translateX(-50%);
            width: 40px;
            height: 40px;
            border: 3px solid #2a2a2a;
            border-top-color: #007BFF;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to {
                transform: translateX(-50%) rotate(360deg);
            }
        }

        /* Scrollbar styling */
        .message-history-container::-webkit-scrollbar,
        .drawer::-webkit-scrollbar {
            width: 6px;
        }

        .message-history-container::-webkit-scrollbar-track,
        .drawer::-webkit-scrollbar-track {
            background: #1a1a1a;
        }

        .message-history-container::-webkit-scrollbar-thumb,
        .drawer::-webkit-scrollbar-thumb {
            background: #4a4a4a;
            border-radius: 3px;
        }

        .message-history-container::-webkit-scrollbar-thumb:hover,
        .drawer::-webkit-scrollbar-thumb:hover {
            background: #5a5a5a;
        }

        .message-history-container::-webkit-scrollbar-thumb:active,
        .drawer::-webkit-scrollbar-thumb:active {
            background: #6a6a6a;
        }
    </style>

    <!-- Live2D Dependencies -->
    <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/dylanNew/live2d/webgl/Live2D/lib/live2d.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pixi.js@6.5.2/dist/browser/pixi.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/eventemitter3@4.0.7/umd/eventemitter3.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pixi-live2d-display@0.4.0/dist/index.min.js"></script>

    <!-- Three.js and VRM dependencies -->
    <script type="module">
        (async () => {
            try {
                const THREE_NS = await import('https://esm.sh/three@0.150.1');
                const { GLTFLoader } = await import('https://esm.sh/three@0.150.1/examples/jsm/loaders/GLTFLoader.js');
                const vrmModule = await import('https://esm.sh/@pixiv/three-vrm@3.4.3?deps=three@0.150.1');
                const { VRMAnimation, VRMAnimationLoaderPlugin } = await import('https://esm.sh/@pixiv/three-vrm-animation@3.4.3?deps=three@0.150.1');

                window.THREE = THREE_NS;
                window.GLTFLoader = GLTFLoader;
                window.VRMLoaderPlugin = vrmModule.VRMLoaderPlugin;
                window.VRMUtils = vrmModule.VRMUtils;
                window.VRMAnimation = VRMAnimation;
                window.VRMAnimationLoaderPlugin = VRMAnimationLoaderPlugin;

                window.__vrmModulesReady = true;
                console.log('VRM modules loaded successfully');
            } catch (error) {
                console.error('Failed to load VRM modules:', error);
                window.__vrmModulesReady = false;
                window.__vrmModulesError = error;
            }
        })();
    </script>
</head>
<body>
    <!-- Header Bar -->
    <div class="header-bar">
        <div class="header-left">
            <button class="icon-btn" id="chat-history-btn" title="Chat History">
                <i class="fas fa-comments"></i>
            </button>
        </div>
        <div class="app-title">EVA</div>
        <div class="header-right">
            <button class="icon-btn" id="settings-btn" title="Settings">
                <i class="fas fa-cog"></i>
            </button>
        </div>
    </div>

    <!-- Main Content -->
    <div class="main-content">
        <!-- Avatar Container -->
        <div class="avatar-container">
            <div id="live2d-container">
                <canvas id="live2d-canvas"></canvas>
            </div>
            <div id="vrm-container" style="display: none;">
                <canvas id="vrm-canvas"></canvas>
            </div>
        </div>

        <!-- Message History -->
        <div class="message-history-container" id="message-history"></div>
    </div>

    <!-- Input Bar -->
    <div class="input-bar">
        <div class="input-wrapper">
            <textarea id="user-input" placeholder="Type your message..." rows="1"></textarea>
        </div>
        <div class="input-actions">
            <button class="action-btn" id="stt-btn" title="Speech to Text">
                <i class="fas fa-microphone"></i>
            </button>
            <button class="action-btn" id="philosopher-btn" title="Philosopher Mode">
                <i class="fas fa-brain"></i>
            </button>
            <button class="action-btn" id="send-btn" title="Send">
                <i class="fas fa-paper-plane"></i>
            </button>
        </div>
    </div>

    <!-- Status Indicator -->
    <div class="status-indicator" id="status-indicator"></div>

    <!-- Drawer Overlay -->
    <div class="drawer-overlay" id="drawer-overlay"></div>

    <!-- Chat History Drawer -->
    <div class="drawer left" id="chat-history-drawer">
        <div class="drawer-header">
            <div class="drawer-title">Conversations</div>
            <button class="drawer-close" id="chat-history-close">
                <i class="fas fa-times"></i>
            </button>
        </div>
        <div class="drawer-content">
            <button class="new-chat-btn" id="new-chat-btn">
                <i class="fas fa-plus"></i> New Chat
            </button>
            <div class="conversation-list" id="conversation-list"></div>
        </div>
    </div>

    <!-- Settings Drawer -->
    <div class="drawer right" id="settings-drawer">
        <div class="drawer-header">
            <div class="drawer-title">Settings</div>
            <button class="drawer-close" id="settings-close">
                <i class="fas fa-times"></i>
            </button>
        </div>
        <div class="drawer-content" id="settings-content">
            <!-- Settings will be populated by JavaScript -->
        </div>
    </div>

    <script>
        // Initialize global variables and expose PIXI
        window.PIXI = PIXI;
        window.EventEmitter3.EventEmitter = EventEmitter3;

        // Server configuration for remote access (same as desktop)
        function getServerBase() {
            const urlParams = new URLSearchParams(window.location.search);
            const serverParam = urlParams.get('server');
            if (serverParam) {
                return serverParam;
            }
            const currentHost = window.location.hostname;
            if (currentHost !== 'localhost' && currentHost !== '127.0.0.1') {
                return currentHost;
            }
            return 'localhost';
        }

        const SERVER_BASE = getServerBase();
        const PROXY_BASE_URL = `http://${SERVER_BASE}:8002`;
        const MCP_BROWSER_BASE_URL = `http://${SERVER_BASE}:5001`;

        // Storage wrapper (same as desktop)
        const storage = {
            data: new Map(),
            isAvailable: false,
            
            init() {
                try {
                    localStorage.setItem('test', 'test');
                    localStorage.removeItem('test');
                    this.isAvailable = true;
                } catch (e) {
                    console.warn('localStorage not available, using in-memory storage');
                    this.isAvailable = false;
                }
            },

            setItem(key, value) {
                if (this.isAvailable) {
                    try {
                        localStorage.setItem(key, value);
                    } catch (e) {
                        console.warn('Error saving to localStorage:', e);
                        this.data.set(key, value);
                    }
                } else {
                    this.data.set(key, value);
                }
            },

            getItem(key) {
                if (this.isAvailable) {
                    try {
                        return localStorage.getItem(key);
                    } catch (e) {
                        console.warn('Error reading from localStorage:', e);
                        return this.data.get(key) || null;
                    }
                } else {
                    return this.data.get(key) || null;
                }
            }
        };

        storage.init();

        // DOM Elements
        const chatHistoryBtn = document.getElementById('chat-history-btn');
        const settingsBtn = document.getElementById('settings-btn');
        const chatHistoryDrawer = document.getElementById('chat-history-drawer');
        const settingsDrawer = document.getElementById('settings-drawer');
        const drawerOverlay = document.getElementById('drawer-overlay');
        const chatHistoryClose = document.getElementById('chat-history-close');
        const settingsClose = document.getElementById('settings-close');
        const userInput = document.getElementById('user-input');
        const sendBtn = document.getElementById('send-btn');
        const sttBtn = document.getElementById('stt-btn');
        const philosopherBtn = document.getElementById('philosopher-btn');
        let voices = []; // Browser voices array
        const SELECTED_VOICE_STORAGE_KEY = 'selectedVoiceURI'; // Persist selected voice across browser sessions

        // Helper function to get settings (must be defined early)
        function getSetting(key, defaultValue = '') {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings[key] !== undefined ? settings[key] : defaultValue;
            } catch {
                return defaultValue;
            }
        }
        const messageHistory = document.getElementById('message-history');
        const statusIndicator = document.getElementById('status-indicator');
        const newChatBtn = document.getElementById('new-chat-btn');
        const conversationList = document.getElementById('conversation-list');
        const settingsContent = document.getElementById('settings-content');

        // State variables
        let displayedMessages = [];
        let chatHistory = [];
        let conversations = [];
        let activeConversationId = null;
        const CONVERSATIONS_STORAGE_KEY = 'conversations';
        const ACTIVE_CONVERSATION_STORAGE_KEY = 'activeConversationId';

        // Drawer management
        let activeDrawer = null;

        function openDrawer(drawer) {
            if (activeDrawer && activeDrawer !== drawer) {
                closeDrawer(activeDrawer);
            }
            drawer.classList.add('active');
            drawerOverlay.classList.add('active');
            activeDrawer = drawer;
            document.body.style.overflow = 'hidden';
        }

        function closeDrawer(drawer) {
            drawer.classList.remove('active');
            drawerOverlay.classList.remove('active');
            if (activeDrawer === drawer) {
                activeDrawer = null;
            }
            if (!activeDrawer) {
                document.body.style.overflow = '';
            }
        }

        function closeAllDrawers() {
            closeDrawer(chatHistoryDrawer);
            closeDrawer(settingsDrawer);
        }

        // Drawer event listeners
        chatHistoryBtn.addEventListener('click', () => openDrawer(chatHistoryDrawer));
        settingsBtn.addEventListener('click', () => openDrawer(settingsDrawer));
        chatHistoryClose.addEventListener('click', () => closeDrawer(chatHistoryDrawer));
        settingsClose.addEventListener('click', () => closeDrawer(settingsDrawer));
        drawerOverlay.addEventListener('click', closeAllDrawers);

        // Status indicator
        function showStatus(message, duration = 3000) {
            statusIndicator.textContent = message;
            statusIndicator.classList.add('active');
            setTimeout(() => {
                statusIndicator.classList.remove('active');
            }, duration);
        }

        // Auto-resize textarea
        userInput.addEventListener('input', function() {
            this.style.height = 'auto';
            this.style.height = Math.min(this.scrollHeight, 120) + 'px';
        });

        // Prevent zoom on input focus (iOS)
        userInput.addEventListener('focus', function() {
            const viewport = document.querySelector('meta[name="viewport"]');
            viewport.setAttribute('content', 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no');
        });

        userInput.addEventListener('blur', function() {
            const viewport = document.querySelector('meta[name="viewport"]');
            viewport.setAttribute('content', 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no');
        });

        // Message history functions (same as desktop)
        function addMessageToHistory(role, content) {
            if (!role || (role !== 'user' && role !== 'assistant' && role !== 'philosopher')) {
                console.error('addMessageToHistory called with invalid role:', role);
                return;
            }
            
            if (typeof content !== 'string') {
                console.error('addMessageToHistory called with invalid content type:', typeof content, content);
                return;
            }
            
            content = content.trim();
            
            const userName = getUserName() || 'User';
            const assistantName = getAssistantName() || 'EVA';
            const philosopherName = assistantName;
            
            const message = {
                role: role,
                content: content,
                sender: role === 'user' ? userName : (role === 'philosopher' ? philosopherName + ' (Contemplating)' : assistantName),
                timestamp: new Date()
            };
            
            displayedMessages.push(message);
            if (displayedMessages.length > 25) {
                displayedMessages.shift();
            }
            
            renderMessageHistory();
            messageHistory.scrollTop = messageHistory.scrollHeight;
            updateActiveConversationMessages();
        }

        function renderMessageHistory() {
            messageHistory.innerHTML = '';
            
            displayedMessages.forEach((msg) => {
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${msg.role}`;
                
                const senderDiv = document.createElement('div');
                senderDiv.className = 'message-sender';
                senderDiv.textContent = msg.sender;
                
                const contentDiv = document.createElement('div');
                contentDiv.className = 'message-content';
                if (msg.content && typeof msg.content === 'string') {
                    contentDiv.textContent = msg.content;
                } else {
                    contentDiv.textContent = '[No content]';
                }
                
                messageDiv.appendChild(senderDiv);
                messageDiv.appendChild(contentDiv);
                messageHistory.appendChild(messageDiv);
            });
        }

        // Placeholder functions for settings (will be implemented)
        function getUserName() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.userName || 'User';
            } catch {
                return 'User';
            }
        }

        function getAssistantName() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.assistantName || 'EVA';
            } catch {
                return 'EVA';
            }
        }

        // Conversation management (same structure as desktop)
        function generateConversationId() {
            return 'conv_' + Date.now() + '_' + Math.random().toString(36).substr(2, 9);
        }

        function createNewConversation(title = null) {
            const newConversation = {
                id: generateConversationId(),
                title: title || 'New Conversation',
                messages: [],
                displayedMessages: [],
                createdAt: new Date().toISOString(),
                updatedAt: new Date().toISOString()
            };
            conversations.push(newConversation);
            saveConversations();
            return newConversation;
        }

        function saveConversations() {
            try {
                storage.setItem(CONVERSATIONS_STORAGE_KEY, JSON.stringify(conversations));
                storage.setItem(ACTIVE_CONVERSATION_STORAGE_KEY, activeConversationId);
            } catch (error) {
                console.warn('Error saving conversations:', error);
            }
        }

        function loadConversations() {
            try {
                const savedConversations = storage.getItem(CONVERSATIONS_STORAGE_KEY);
                const savedActiveId = storage.getItem(ACTIVE_CONVERSATION_STORAGE_KEY);
                
                if (savedConversations) {
                    conversations = JSON.parse(savedConversations);
                }
                
                if (conversations.length === 0) {
                    const firstConversation = createNewConversation('Welcome Chat');
                    activeConversationId = firstConversation.id;
                } else if (savedActiveId && conversations.find(c => c.id === savedActiveId)) {
                    activeConversationId = savedActiveId;
                } else {
                    activeConversationId = conversations[0].id;
                }
                
                switchToConversation(activeConversationId, true);
                renderConversationList();
            } catch (error) {
                console.warn('Error loading conversations:', error);
                const firstConversation = createNewConversation('Welcome Chat');
                activeConversationId = firstConversation.id;
                renderConversationList();
            }
        }

        function getActiveConversation() {
            return conversations.find(c => c.id === activeConversationId);
        }

        function switchToConversation(conversationId, skipTimestampUpdate = false) {
            const currentConv = getActiveConversation();
            if (currentConv) {
                const messagesChanged = JSON.stringify(currentConv.messages) !== JSON.stringify(chatHistory) ||
                                      JSON.stringify(currentConv.displayedMessages) !== JSON.stringify(displayedMessages);
                
                currentConv.messages = [...chatHistory];
                currentConv.displayedMessages = [...displayedMessages];
                
                if (!skipTimestampUpdate && messagesChanged) {
                    currentConv.updatedAt = new Date().toISOString();
                }
            }
            
            activeConversationId = conversationId;
            const newConv = getActiveConversation();
            
            if (newConv) {
                chatHistory = [...newConv.messages];
                displayedMessages = [...newConv.displayedMessages];
                renderMessageHistory();
                messageHistory.scrollTop = messageHistory.scrollHeight;
            }
            
            saveConversations();
            renderConversationList();
        }

        function deleteConversation(conversationId) {
            const index = conversations.findIndex(c => c.id === conversationId);
            if (index !== -1) {
                conversations.splice(index, 1);
                
                if (conversationId === activeConversationId) {
                    if (conversations.length === 0) {
                        const newConv = createNewConversation('New Chat');
                        activeConversationId = newConv.id;
                    } else {
                        activeConversationId = conversations[0].id;
                    }
                    switchToConversation(activeConversationId);
                }
                
                saveConversations();
                renderConversationList();
            }
        }

        function renderConversationList() {
            conversationList.innerHTML = '';
            
            const sortedConversations = [...conversations].sort((a, b) => 
                new Date(b.updatedAt) - new Date(a.updatedAt)
            );
            
            sortedConversations.forEach(conv => {
                const convItem = document.createElement('div');
                convItem.className = 'conversation-item' + (conv.id === activeConversationId ? ' active' : '');
                convItem.dataset.conversationId = conv.id;
                
                const titleDiv = document.createElement('div');
                titleDiv.className = 'conversation-title';
                titleDiv.textContent = conv.title;
                
                const dateDiv = document.createElement('div');
                dateDiv.className = 'conversation-date';
                const date = new Date(conv.updatedAt);
                dateDiv.textContent = date.toLocaleDateString() + ' ' + date.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'});
                
                convItem.appendChild(titleDiv);
                convItem.appendChild(dateDiv);
                
                convItem.addEventListener('click', () => {
                    if (conv.id !== activeConversationId) {
                        switchToConversation(conv.id);
                        closeDrawer(chatHistoryDrawer);
                    }
                });
                
                conversationList.appendChild(convItem);
            });
        }

        function updateActiveConversationMessages() {
            const activeConv = getActiveConversation();
            if (activeConv) {
                activeConv.messages = [...chatHistory];
                activeConv.displayedMessages = [...displayedMessages];
                activeConv.updatedAt = new Date().toISOString();
                
                if (activeConv.title === 'New Conversation' && chatHistory.length > 0) {
                    const firstUserMsg = chatHistory.find(msg => msg.role === 'user');
                    if (firstUserMsg && firstUserMsg.content) {
                        const title = firstUserMsg.content.substring(0, 30) + (firstUserMsg.content.length > 30 ? '...' : '');
                        activeConv.title = title;
                    }
                }
                
                saveConversations();
                renderConversationList();
            }
        }

        newChatBtn.addEventListener('click', () => {
            const newConv = createNewConversation();
            activeConversationId = newConv.id;
            switchToConversation(newConv.id);
            closeDrawer(chatHistoryDrawer);
        });

        // Initialize conversations on load
        loadConversations();

        // ============================================================================
        // AVATAR INITIALIZATION (Live2D and VRM)
        // ============================================================================
        
        let live2dModel;
        let live2dTickerRegistered = false;
        let live2dOffsets = {};
        let modelPath = './model_avatar/RACOON01/RACOON01.model3.json';
        
        let vrmModel;
        let vrmScene;
        let vrmCamera;
        let vrmRenderer;
        let vrmMixer;
        let vrmClock;
        let lovePoseTimeoutId = null;
        let isSpeaking = false;
        let isMuted = false; // TTS mute flag
        let lovePoseWeight = 0;
        // TTS and lip sync variables
        let ttsLipSyncIntervalId = null; // Interval used by SpeechSynthesis fallback
        let ttsRafId = 0; // requestAnimationFrame id for lip sync
        let ttsCleanupFns = []; // Cleanup functions for TTS
        let ttsAbortController = null; // AbortController for cancelling TTS requests
        let ttsGeneration = 0; // Generation token for TTS requests
        let ttsStreamActive = false; // Tracks if TTS stream is active
        let ttsAnalyserNode = null; // Shared analyser used for PCM16 streaming lip sync
        let ttsAnalyserLoopActive = false; // Tracks whether analyser-driven lip sync loop is running
        // Microsoft TTS lip sync variables (smooth amplitude-based approach)
        let microsoftTtsRafId = 0; // requestAnimationFrame id for Microsoft TTS lip sync loop
        let microsoftTtsTargetAmplitude = 0; // Target amplitude for smooth lip sync (0-1)
        let microsoftTtsSmoothedAmplitude = 0; // Current smoothed amplitude value
        let microsoftTtsLastBoundaryTs = 0; // Timestamp of last boundary event
        let microsoftTtsIsActive = false; // Flag to track if Microsoft TTS lip sync is active
        let targetLovePoseWeight = 0;
        let thinkPoseTimeoutId = null;
        let thinkPoseWeight = 0;
        let vrmVersion = '1.0';
        let targetThinkPoseWeight = 0;
        let vrmCryPoseActive = false;
        let cryPoseTimeoutId = null;
        let cryPoseWeight = 0;
        let targetCryPoseWeight = 0;
        let vrmAngryPoseActive = false;
        let angryPoseTimeoutId = null;
        let angryPoseWeight = 0;
        let targetAngryPoseWeight = 0;
        let vrmPositions = {};
        let currentVRMModelPath = '';
        let vrmBlinkTimeout = null; // Timer id used to schedule periodic blinking
        const L2D_SELECTED_KEY = 'live2dSelectedModelPath';
        const VRM_SELECTED_KEY = 'vrmSelectedModelPath';
        let vrmLoveVrmaAction = null; // Prepared AnimationAction for love VRMA
        let vrmThinkVrmaAction = null; // Prepared AnimationAction for thinking VRMA
        let vrmCryVrmaAction = null; // Prepared AnimationAction for cry VRMA
        let vrmAngryVrmaAction = null; // Prepared AnimationAction for angry VRMA
        let vrmLovePoseActive = false; // Whether the love pose is active
        let vrmThinkPoseActive = false; // Whether the thinking pose is active
        let vrmLipSyncMorphTarget = null; // Morph target key for lip sync
        let lookTarget = null; // 3D object used as look target for head/eye tracking
        let lastMouseMoveTs = performance.now(); // Timestamp of the most recent mouse move
        
        // Pose configuration (tweak these numbers to adjust poses)
        const POSE_CONFIG = {
            blendSmoothing: 0.12, // Easing toward target pose weights per frame
            love: {
                durationMs: 6000, // How long to hold the love pose
                expressionsOnly: true, // Do not apply manual limb rotations; VRMA drives motion
                vrmaPath: './model_avatar/Eva/Kawaii Kaiwai.vrma', // VRMA animation path for love pose
                useVrma: true, // Prefer VRMA over JSON pose
                upperArmRollFactor: 0.15, // Reduce side roll by this fraction of armLowering
                upperArmPitchForward: 0.9, // How much to pitch both upper arms forward (negative X)
                upperArmYawIn: 0.9, // How much to yaw both upper arms inward (Y)
                forearmBend: 1.4, // How much to bend both elbows (forearm Z)
                handYawIn: 1.1, // How much to yaw both hands inward (Y)
                smileGain: 0.85, // Smile amplitude scale during pose
                smileBias: 0.2, // Extra bias so smile starts visible
                loveEyesGain: 0.75, // Love eyes (relaxed) amplitude scale during pose
                loveEyesBias: 0.15, // Extra bias so love eyes start visible
                affectFace: false, // If true, also set smile/eye shapes (disabled per user request)
                poseJsonPath: 'model_avatar/Eva/Eva0.vrm.json', // Optional pose JSON path with target quaternions (ignored when useVrma is true)
                poseName: 'Heart', // Pose name in the JSON file
                convertUnityQuat: false // Do not flip axes; VRM Poser quaternions align with three-vrm normalized bones
            },
            think: {
                durationMs: 6000, // How long to hold the thinking pose
                expressionsOnly: true, // If true, only drive facial expressions; skip limb rotations
                vrmaPath: './model_avatar/Eva/VRMA_06.vrma', // VRMA animation path for thinking pose
                useVrma: true, // Prefer VRMA over manual pose
                upperArmPitchForward: 0.6, // Additional forward pitch for right upper arm
                upperArmYawIn: 0.65, // Additional inward yaw for right upper arm
                upperArmRollZ: 0.2, // Additional roll for right upper arm
                forearmBendExtra: 1.6, // Extra elbow bend for right forearm
                handYawInExtra: 0.8, // Extra inward yaw on right hand
                oMouthGain: 0.75, // O mouth amplitude scale
                oMouthBias: 0.2, // O mouth bias
                browRaiseGain: 0.6 // Brow raise amplitude scale
            },
            cry: {
                durationMs: 6000, // How long to hold the cry pose
                expressionsOnly: true, // Only drive facial expressions; skip limb rotations
                vrmaPath: './model_avatar/Eva/007_gekirei.vrma', // VRMA animation path for cry pose
                useVrma: true, // Prefer VRMA over manual pose
                sadMouthGain: 0.75, // Sad mouth amplitude scale
                sadMouthBias: 0.2, // Sad mouth bias
                tearGain: 0.8 // Tear/crying expression amplitude
            },
            angry: {
                durationMs: 6000, // How long to hold the angry pose
                expressionsOnly: true, // Only drive facial expressions; skip limb rotations
                vrmaPath: './model_avatar/Eva/VRMA_04.vrma', // VRMA animation path for angry pose
                useVrma: true // Prefer VRMA over manual pose
            }
        };
        
        let audioContextResumed = false;
        
        async function resumeAudioContextOnce() {
            if (audioContextResumed) {
                // Even if already called, ensure TTS audio context is resumed
                if (window.__ttsAudioContext && window.__ttsAudioContext.state === 'suspended') {
                    try {
                        await window.__ttsAudioContext.resume();
                        console.log('✅ TTS Audio context resumed');
                    } catch (e) {
                        console.warn('⚠️ Failed to resume TTS audio context:', e);
                    }
                }
                return;
            }
            audioContextResumed = true;
            try {
                if (window.__opus?.audioCtx && window.__opus.audioCtx.state === 'suspended') {
                    await window.__opus.resume();
                    console.log('✅ Audio context resumed (Opus decoder)');
                }
                // Also resume TTS audio context if it exists
                if (window.__ttsAudioContext && window.__ttsAudioContext.state === 'suspended') {
                    await window.__ttsAudioContext.resume();
                    console.log('✅ TTS Audio context resumed');
                }
            } catch (e) {
                console.warn('⚠️ Failed to resume audio context:', e);
            }
        }

        async function initLive2D() {
            try {
                if (document.readyState !== 'complete') {
                    await new Promise(resolve => window.addEventListener('load', resolve));
                }

                // Load saved offsets
                try {
                    const L2D_OFFSETS_KEY = 'live2dOffsets';
                    const savedOffsets = storage.getItem(L2D_OFFSETS_KEY);
                    if (savedOffsets) {
                        live2dOffsets = JSON.parse(savedOffsets);
                    }
                } catch (e) {
                    console.warn('Error loading Live2D offsets:', e);
                }

                const container = document.getElementById('live2d-container');
                const canvas = document.getElementById('live2d-canvas');
                if (!container || !canvas) {
                    console.warn('Live2D container/canvas not found. Skipping init.');
                    return;
                }
                
                const app = new PIXI.Application({
                    view: canvas,
                    transparent: true,
                    autoStart: true,
                    width: container.clientWidth,
                    height: container.clientHeight
                });

                if (!live2dTickerRegistered) {
                    await PIXI.live2d.Live2DModel.registerTicker(PIXI.Ticker);
                    live2dTickerRegistered = true;
                }

                const model = await PIXI.live2d.Live2DModel.from(modelPath, {
                    autoInteract: false,
                    focus: false
                });
                
                app.stage.removeChildren();
                app.stage.addChild(model);

                const resizeModel = () => {
                    app.renderer.resize(container.clientWidth, container.clientHeight);
                    const scale = Math.min(
                        container.clientWidth / (model.width * 1.5),
                        container.clientHeight / (model.height * 1.5)
                    ) * 2.5;
                    model.scale.set(scale);
                    model.x = container.clientWidth / 2;
                    const baseY = container.clientHeight / 1.4;
                    const offsetPx = live2dOffsets[modelPath] ?? 0;
                    model.y = baseY + offsetPx;
                };

                model.anchor.set(0.5, 0.4);
                resizeModel();

                const debouncedResize = (() => {
                    let raf = null;
                    return () => {
                        if (raf) cancelAnimationFrame(raf);
                        raf = requestAnimationFrame(resizeModel);
                    };
                })();
                window.addEventListener('resize', debouncedResize);
                model.__app = app;
                model.__resizeHandler = debouncedResize;

                model.draggable = false;
                model.following = false;
                model.interactive = false;
                model.tracking = false;
                model.removeAllListeners();
                
                if (model.internalModel) {
                    model.internalModel.coreModel.setParameterValueById('ParamAngleX', 0);
                    model.internalModel.coreModel.setParameterValueById('ParamAngleY', 0);
                    model.internalModel.coreModel.setParameterValueById('ParamAngleZ', 0);
                    model.internalModel.coreModel.setParameterValueById('ParamEyeBallX', 0);
                    model.internalModel.coreModel.setParameterValueById('ParamEyeBallY', 0);
                }

                live2dModel = model;
                console.log('Live2D model loaded successfully');
            } catch (error) {
                console.error('Failed to load Live2D model:', error);
            }
        }

        function cleanupLive2D() {
            try {
                if (!live2dModel) return;
                if (live2dModel.__resizeHandler) {
                    window.removeEventListener('resize', live2dModel.__resizeHandler);
                }
                if (live2dModel.__app) {
                    try {
                        live2dModel.__app.destroy(false, { children: true, texture: true, baseTexture: true });
                    } catch {}
                }
                if (typeof live2dModel.destroy === 'function') {
                    try {
                        live2dModel.destroy({ children: true, texture: true, baseTexture: true });
                    } catch {}
                }
            } catch (err) {
                console.warn('cleanupLive2D encountered an issue:', err);
            } finally {
                live2dModel = null;
            }
        }

        function updateVRMTransform() {
            if (!vrmModel || !vrmModel.scene) return;

            const currentPositions = vrmPositions[currentVRMModelPath] || { scale: 1.0, positionX: 0, positionY: 0, rotation: 0 };

            // Apply scale
            vrmModel.scene.scale.setScalar(currentPositions.scale);

            // Apply position
            vrmModel.scene.position.set(currentPositions.positionX, currentPositions.positionY, 0);

            // Apply rotation
            vrmModel.scene.rotation.y = (currentPositions.rotation * Math.PI) / 180;

            // Persist the positions
            vrmPositions[currentVRMModelPath] = currentPositions;
            try { 
                const VRM_POSITIONS_KEY = 'vrmPositions';
                storage.setItem(VRM_POSITIONS_KEY, JSON.stringify(vrmPositions)); 
            } catch {}
            // If renderer exists, render a frame to reflect changes immediately
            if (vrmRenderer && vrmScene && vrmCamera) {
                try { vrmRenderer.render(vrmScene, vrmCamera); } catch (_) {}
            }
        }

        function animateVRMLipSync(value) {
            // Ensure VRM model exists and we have a target key
            if (!vrmModel || !vrmLipSyncMorphTarget) return;

            // Normalize key; allow both VRM 0.x ('A') and VRM 1.0 ('aa') conventions
            let key = vrmLipSyncMorphTarget;
            if (typeof key !== 'string') {
                key = (key && (key.presetName || key.name)) || '';
            }
            if (!key) return;

            const clamped = Math.max(0, Math.min(1, value));

            // Try VRM 1.0 first if available
            try {
                if (vrmModel.expressionManager) {
                    // For VRM 1.0, vowel keys are typically lowercase: 'aa','ih','ou','ee','oh'
                    const exprKey = key.toLowerCase() === 'a' ? 'aa' : key.toLowerCase();
                    vrmModel.expressionManager.setValue(exprKey, clamped);
                    return;
                }
            } catch (e) {
                // fall through to VRM 0.x
            }

            // Fallback: VRM 0.x blendShapeProxy uses presets like 'A','I','U','E','O' or custom names
            try {
                if (vrmModel.blendShapeProxy) {
                    const proxyKey = key.length === 2 && key === key.toLowerCase() ? key.toUpperCase().charAt(0) : key; // map 'aa'->'A'
                    vrmModel.blendShapeProxy.setValue(proxyKey, clamped);
                }
            } catch (error) {
                console.warn('Error animating VRM lip sync:', error);
            }
        }

        function cleanupVRM() {
            try {
                if (!vrmModel) return;

                // Stop and uncache animation bindings
                try {
                    if (vrmMixer && vrmModel && vrmModel.scene) {
                        vrmMixer.stopAllAction();
                        vrmMixer.uncacheRoot(vrmModel.scene);
                    }
                } catch (_) {}

                if (vrmScene) {
                    vrmScene.traverse((child) => {
                        if (child.geometry) child.geometry.dispose();
                        if (child.material) {
                            if (Array.isArray(child.material)) {
                                child.material.forEach(material => material.dispose());
                            } else {
                                child.material.dispose();
                            }
                        }
                    });
                }

                if (vrmRenderer) {
                    vrmRenderer.dispose();
                }

                vrmModel = null;
                vrmScene = null;
                vrmCamera = null;
                vrmRenderer = null;
                vrmMixer = null;
                vrmClock = null;
                vrmLipSyncMorphTarget = null;
                if (vrmBlinkTimeout) { try { clearTimeout(vrmBlinkTimeout); } catch (_) {} vrmBlinkTimeout = null; }
                if (lovePoseTimeoutId) { try { clearTimeout(lovePoseTimeoutId); } catch (_) {} lovePoseTimeoutId = null; }
                if (thinkPoseTimeoutId) { try { clearTimeout(thinkPoseTimeoutId); } catch (_) {} thinkPoseTimeoutId = null; }
                if (cryPoseTimeoutId) { try { clearTimeout(cryPoseTimeoutId); } catch (_) {} cryPoseTimeoutId = null; }
                if (angryPoseTimeoutId) { try { clearTimeout(angryPoseTimeoutId); } catch (_) {} angryPoseTimeoutId = null; }

            } catch (error) {
                console.warn('Error cleaning up VRM:', error);
            }
        }

        // Avatar mode switching functions
        async function switchToLive2D() {
            // Hide VRM container
            const vrmContainer = document.getElementById('vrm-container');
            if (vrmContainer) vrmContainer.style.display = 'none';

            // Show Live2D container
            const live2dContainer = document.getElementById('live2d-container');
            if (live2dContainer) live2dContainer.style.display = 'block';

            // Cleanup VRM if active
            cleanupVRM();

            // Initialize Live2D if needed
            if (!live2dModel) {
                await initLive2D();
            }

            // Persist avatar mode preference
            try { 
                storage.setItem('avatarMode', 'live2d');
                const currentSettings = JSON.parse(storage.getItem('toolSettings') || '{}');
                currentSettings.avatarMode = 'live2d';
                storage.setItem('toolSettings', JSON.stringify(currentSettings));
            } catch {}
        }

        async function switchToVRM() {
            // Hide Live2D container
            const live2dContainer = document.getElementById('live2d-container');
            if (live2dContainer) live2dContainer.style.display = 'none';

            // Show VRM container
            const vrmContainer = document.getElementById('vrm-container');
            if (vrmContainer) vrmContainer.style.display = 'block';

            // Cleanup Live2D if active
            cleanupLive2D();

            // Initialize VRM if needed
            if (!vrmModel) {
                await initVRM();
            }

            // Persist avatar mode preference
            try { 
                storage.setItem('avatarMode', 'vrm');
                const currentSettings = JSON.parse(storage.getItem('toolSettings') || '{}');
                currentSettings.avatarMode = 'vrm';
                storage.setItem('toolSettings', JSON.stringify(currentSettings));
            } catch {}
        }

        // VRM initialization (simplified version for mobile)
        async function initVRM() {
            try {
                const container = document.getElementById('vrm-container');
                const canvas = document.getElementById('vrm-canvas');
                if (!container || !canvas) {
                    console.warn('VRM container/canvas not found. Skipping init.');
                    return;
                }

                // Wait for VRM modules to be ready
                if (!window.__vrmModulesReady || !window.THREE || !window.GLTFLoader || !window.VRMLoaderPlugin) {
                    await new Promise((resolve) => {
                        const start = Date.now();
                        const timer = setInterval(() => {
                            if (window.__vrmModulesReady && window.THREE && window.GLTFLoader && window.VRMLoaderPlugin) {
                                clearInterval(timer);
                                resolve();
                            } else if (Date.now() - start > 5000) {
                                clearInterval(timer);
                                resolve();
                            }
                        }, 50);
                    });
                }

                if (!window.THREE || !window.GLTFLoader) {
                    console.error('THREE.js or GLTFLoader not available');
                    return;
                }

                // Get current VRM model path
                let selectedVRMPath = currentVRMModelPath;
                if (!selectedVRMPath) {
                    try {
                        const VRM_SELECTED_KEY_LOCAL = 'vrmSelectedModelPath';
                        selectedVRMPath = storage.getItem(VRM_SELECTED_KEY_LOCAL) || getSetting('vrmSelectedModel', '');
                    } catch (e) {
                        selectedVRMPath = getSetting('vrmSelectedModel', '');
                    }
                }

                if (!selectedVRMPath) {
                    console.warn('No VRM model path selected');
                    return;
                }

                // Initialize Three.js scene
                const scene = new window.THREE.Scene();
                const camera = new window.THREE.PerspectiveCamera(45, canvas.clientWidth / canvas.clientHeight, 0.1, 1000);
                const renderer = new window.THREE.WebGLRenderer({ canvas: canvas, antialias: true });
                renderer.setSize(canvas.clientWidth, canvas.clientHeight);
                renderer.setClearColor(0x000000, 0);

                // Add lighting
                const ambientLight = new window.THREE.AmbientLight(0x404040, 0.6);
                scene.add(ambientLight);
                const directionalLight = new window.THREE.DirectionalLight(0xffffff, 0.8);
                directionalLight.position.set(1, 1, 1);
                scene.add(directionalLight);

                // Load VRM model
                const loader = new window.GLTFLoader();
                if (window.VRMLoaderPlugin) {
                    loader.register(parser => new window.VRMLoaderPlugin(parser));
                }
                if (vrmVersion === '1.0' && window.VRMAnimationLoaderPlugin) {
                    loader.register(parser => new window.VRMAnimationLoaderPlugin(parser));
                }

                const gltf = await new Promise((resolve, reject) => {
                    loader.load(
                        selectedVRMPath,
                        resolve,
                        undefined,
                        reject
                    );
                });

                // Extract VRM instance
                let vrm = null;
                if (gltf && gltf.userData && gltf.userData.vrm) {
                    vrm = gltf.userData.vrm;
                } else if (gltf && gltf.parser && gltf.parser.userData && gltf.parser.userData.vrm) {
                    vrm = gltf.parser.userData.vrm;
                } else if (gltf && gltf.scene && gltf.scene.userData && gltf.scene.userData.vrm) {
                    vrm = gltf.scene.userData.vrm;
                }
                
                if (!vrm) {
                    throw new Error('Loaded GLTF does not contain VRM data.');
                }

                // Optimize skeleton
                if (window.VRMUtils && vrm && vrm.scene) {
                    try { window.VRMUtils.combineSkeletons(vrm.scene); } catch (_) {}
                }
                
                // Handle VRM 0.0 orientation
                if (vrmVersion === '0.0' && window.VRMUtils && vrm) {
                    try {
                        if (vrm.meta && vrm.meta.metaVersion === '0') {
                            window.VRMUtils.rotateVRM0(vrm);
                        }
                    } catch (e) {
                        console.warn('Could not rotate VRM 0.0 model:', e);
                    }
                }

                // Find lip sync morph target (supports VRM 0.x and VRM 1.0)
                try {
                    if (vrm.expressionManager) {
                        // VRM 1.0: use vowel expression key 'aa' if available, fallback to common names
                        vrmLipSyncMorphTarget = 'aa';
                    } else if (vrm.blendShapeProxy) {
                        // VRM 0.x: use preset 'A' if available, fallback by name
                        const groups = vrm.blendShapeProxy.getBlendShapeGroupList ? vrm.blendShapeProxy.getBlendShapeGroupList() : [];
                        const findByPreset = (preset) => groups.find(g => String(g.presetName || '').toUpperCase() === String(preset).toUpperCase());
                        const findByName = (regex) => groups.find(g => regex.test(String(g.name || '')));
                        const aPreset = findByPreset('A');
                        if (aPreset) {
                            vrmLipSyncMorphTarget = 'A';
                        } else {
                            const mouthOpen = findByName(/MouthOpen/i);
                            const anyMouth = mouthOpen || findByName(/Mouth/i) || findByName(/Lip/i);
                            vrmLipSyncMorphTarget = anyMouth ? anyMouth.name : null;
                        }
                    } else {
                        vrmLipSyncMorphTarget = null;
                    }
                } catch (e) {
                    console.warn('Failed to resolve VRM lip sync target:', e);
                    vrmLipSyncMorphTarget = null;
                }

                scene.add(vrm.scene);
                
                // Position camera
                camera.position.set(0, 0, 5);
                camera.lookAt(0, 0, 0);

                // Animation mixer
                const mixer = new window.THREE.AnimationMixer(vrm.scene);
                const clock = new window.THREE.Clock();

                // Create a look-at target for head/eye tracking
                lookTarget = new window.THREE.Object3D();
                scene.add(lookTarget);
                if (vrm.lookAt) {
                    vrm.lookAt.target = lookTarget;
                }
                lastMouseMoveTs = performance.now();

                // Prepare bone references for idle animation
                const getBone = (name) => {
                    if (vrm.humanoid && typeof vrm.humanoid.getNormalizedBoneNode === 'function') {
                        return vrm.humanoid.getNormalizedBoneNode(name);
                    }
                    if (vrm.humanoid && typeof vrm.humanoid.getRawBoneNode === 'function') {
                        return vrm.humanoid.getRawBoneNode(name);
                    }
                    return null;
                };
                const leftUpperArm = getBone('leftUpperArm');
                const rightUpperArm = getBone('rightUpperArm');
                const leftLowerArm = getBone('leftLowerArm');
                const rightLowerArm = getBone('rightLowerArm');
                const spineBone = getBone('spine');
                const neckBone = getBone('neck');
                const headBone = getBone('head');
                const leftHand = getBone('leftHand');
                const rightHand = getBone('rightHand');

                // Capture base rotations for idle animation
                const baseLeftArmZ = leftUpperArm ? leftUpperArm.rotation.z : 0;
                const baseRightArmZ = rightUpperArm ? rightUpperArm.rotation.z : 0;
                const baseLeftArmY = leftUpperArm ? leftUpperArm.rotation.y : 0;
                const baseRightArmY = rightUpperArm ? rightUpperArm.rotation.y : 0;
                const baseLeftUpperArmX = leftUpperArm ? leftUpperArm.rotation.x : 0;
                const baseLeftLowerArmZ = leftLowerArm ? leftLowerArm.rotation.z : 0;
                const baseRightLowerArmZ = rightLowerArm ? rightLowerArm.rotation.z : 0;
                const baseSpineY = spineBone ? spineBone.position.y : 0;
                const baseNeckX = neckBone ? neckBone.rotation.x : 0;
                const baseNeckY = neckBone ? neckBone.rotation.y : 0;
                const baseHeadX = headBone ? headBone.rotation.x : 0;
                const baseHeadY = headBone ? headBone.rotation.y : 0;
                const baseLeftHandY = leftHand ? leftHand.rotation.y : 0;
                const baseRightHandY = rightHand ? rightHand.rotation.y : 0;
                const baseRightUpperArmX = rightUpperArm ? rightUpperArm.rotation.x : 0;
                const armLowering = 1.25;
                const armInward = 0.18;
                const elbowBend = 0.25;
                const idleStart = performance.now();

                // Store references before animation loop
                vrmModel = vrm;
                vrmScene = scene;
                vrmCamera = camera;
                vrmRenderer = renderer;
                vrmMixer = mixer;
                vrmClock = clock;
                currentVRMModelPath = selectedVRMPath;

                // Load and apply saved positions for this model
                try {
                    const VRM_POSITIONS_KEY = 'vrmPositions';
                    const savedPositions = storage.getItem(VRM_POSITIONS_KEY);
                    if (savedPositions) {
                        vrmPositions = JSON.parse(savedPositions);
                    }
                } catch (e) {
                    console.warn('Error loading VRM positions:', e);
                }
                updateVRMTransform();

                // Full animation loop with idle animations
                function animate() {
                    requestAnimationFrame(animate);

                    if (vrmClock) {
                        const delta = vrmClock.getDelta();
                        // Update VRM BEFORE mixer so it doesn't override animation
                        if (vrmModel && typeof vrmModel.update === 'function') {
                            try { vrmModel.update(delta); } catch (_) {}
                        }
                        // Update mixer AFTER VRM so animation has final control over bones
                        if (vrmMixer) {
                            vrmMixer.update(delta);
                        }

                        // Idle head/eye movement by drifting the look target when idle
                        if (lookTarget && vrmCamera) {
                            const idleFor = performance.now() - lastMouseMoveTs;
                            if (idleFor > 1500) {
                                const forward = new window.THREE.Vector3();
                                vrmCamera.getWorldDirection(forward).normalize();
                                const worldUp = new window.THREE.Vector3(0, 1, 0);
                                const right = new window.THREE.Vector3().crossVectors(forward, worldUp).normalize();
                                const up = new window.THREE.Vector3().crossVectors(right, forward).normalize();
                                const t = performance.now() * 0.001;
                                const dx = Math.sin(t * 0.6) * 0.25 + Math.sin(t * 1.1 + 1.7) * 0.08;
                                const dy = Math.sin(t * 0.8 + 0.5) * 0.18 + Math.sin(t * 1.3 + 2.2) * 0.06;
                                const base = new window.THREE.Vector3().copy(vrmCamera.position).add(forward.multiplyScalar(2.0));
                                const targetPos = new window.THREE.Vector3().copy(base).add(right.multiplyScalar(dx)).add(up.multiplyScalar(dy));
                                lookTarget.position.lerp(targetPos, 0.05);
                            }
                        }

                        // Blend weights toward targets for poses
                        lovePoseWeight += (targetLovePoseWeight - lovePoseWeight) * POSE_CONFIG.blendSmoothing;
                        thinkPoseWeight += (targetThinkPoseWeight - thinkPoseWeight) * POSE_CONFIG.blendSmoothing;

                        // Apply lightweight idle motion each frame
                        const t = (performance.now() - idleStart) / 1000;
                        const sway = Math.sin(t * 1.1) * 0.035;
                        const breathe = Math.sin(t * 2.0) * 0.01;
                        const inwardSway = Math.sin(t * 0.7) * 0.03;
                        const elbowSway = Math.sin(t * 1.7) * 0.06;

                        // Apply idle animations to arms (only if VRMA animations are not running)
                        if (!(vrmLoveVrmaAction && vrmLoveVrmaAction.isRunning()) && 
                            !(vrmThinkVrmaAction && vrmThinkVrmaAction.isRunning()) && 
                            !(vrmCryVrmaAction && vrmCryVrmaAction.isRunning()) && 
                            !(vrmAngryVrmaAction && vrmAngryVrmaAction.isRunning())) {
                            
                            if (vrmVersion === '0.0' && vrm && vrm.meta && vrm.meta.metaVersion === '0') {
                                // VRM 0.0: Set arms to natural rest pose
                                try {
                                    if (leftUpperArm) {
                                        leftUpperArm.quaternion.setFromAxisAngle(
                                            new window.THREE.Vector3(0, 1, 0), 
                                            Math.PI * 0.05
                                        );
                                    }
                                    if (rightUpperArm) {
                                        rightUpperArm.quaternion.setFromAxisAngle(
                                            new window.THREE.Vector3(0, 1, 0), 
                                            -Math.PI * 0.05
                                        );
                                    }
                                    if (leftLowerArm) {
                                        leftLowerArm.quaternion.setFromAxisAngle(
                                            new window.THREE.Vector3(0, 0, 1), 
                                            Math.PI * 0.02
                                        );
                                    }
                                    if (rightLowerArm) {
                                        rightLowerArm.quaternion.setFromAxisAngle(
                                            new window.THREE.Vector3(0, 0, 1), 
                                            -Math.PI * 0.02
                                        );
                                    }
                                } catch (e) {}
                            } else {
                                // VRM 1.0: Apply idle animations with pose blending
                                const w = lovePoseWeight;
                                const wt = thinkPoseWeight;
                                
                                if (leftUpperArm) {
                                    const defZ = baseLeftArmZ - armLowering + sway;
                                    const tgtZ = baseLeftArmZ - armLowering * POSE_CONFIG.love.upperArmRollFactor;
                                    leftUpperArm.rotation.z = defZ * (1 - w) + tgtZ * w;
                                    const defY = baseLeftArmY + armInward + inwardSway;
                                    const tgtY = baseLeftArmY + POSE_CONFIG.love.upperArmYawIn;
                                    leftUpperArm.rotation.y = defY * (1 - w) + tgtY * w;
                                    const defX = baseLeftUpperArmX;
                                    const tgtX = baseLeftUpperArmX - POSE_CONFIG.love.upperArmPitchForward;
                                    leftUpperArm.rotation.x = defX * (1 - w) + tgtX * w;
                                }
                                
                                if (rightUpperArm) {
                                    const defZ = baseRightArmZ + armLowering - sway;
                                    const tgtZ = baseRightArmZ + armLowering * POSE_CONFIG.love.upperArmRollFactor;
                                    let outZ = defZ * (1 - w) + tgtZ * w;
                                    const defY = baseRightArmY - armInward - inwardSway;
                                    const tgtY = baseRightArmY - POSE_CONFIG.love.upperArmYawIn;
                                    let outY = defY * (1 - w) + tgtY * w;
                                    const defX = baseRightUpperArmX;
                                    const tgtX = baseRightUpperArmX - POSE_CONFIG.love.upperArmPitchForward;
                                    let outX = defX * (1 - w) + tgtX * w;
                                    
                                    if (wt > 0 && !POSE_CONFIG.think.expressionsOnly) {
                                        const tRaise = baseRightUpperArmX - POSE_CONFIG.think.upperArmPitchForward;
                                        const tYawIn = baseRightArmY - POSE_CONFIG.think.upperArmYawIn;
                                        const tRoll = baseRightArmZ + POSE_CONFIG.think.upperArmRollZ;
                                        outX = outX * (1 - wt) + tRaise * wt;
                                        outY = outY * (1 - wt) + tYawIn * wt;
                                        outZ = outZ * (1 - wt) + tRoll * wt;
                                    }
                                    rightUpperArm.rotation.x = outX;
                                    rightUpperArm.rotation.y = outY;
                                    rightUpperArm.rotation.z = outZ;
                                }
                                
                                if (leftLowerArm) {
                                    const def = baseLeftLowerArmZ + elbowBend + elbowSway;
                                    const tgt = baseLeftLowerArmZ + POSE_CONFIG.love.forearmBend;
                                    leftLowerArm.rotation.z = def * (1 - w) + tgt * w;
                                }
                                
                                if (rightLowerArm) {
                                    const def = baseRightLowerArmZ + elbowBend - elbowSway;
                                    const tgt = baseRightLowerArmZ + POSE_CONFIG.love.forearmBend;
                                    let out = def * (1 - w) + tgt * w;
                                    if (wt > 0 && !POSE_CONFIG.think.expressionsOnly) {
                                        const tFore = baseRightLowerArmZ + POSE_CONFIG.think.forearmBendExtra;
                                        out = out * (1 - wt) + tFore * wt;
                                    }
                                    rightLowerArm.rotation.z = out;
                                }
                                
                                if (leftHand) {
                                    const def = baseLeftHandY;
                                    const tgt = baseLeftHandY + POSE_CONFIG.love.handYawIn;
                                    leftHand.rotation.y = def * (1 - w) + tgt * w;
                                }
                                
                                if (rightHand) {
                                    const def = baseRightHandY;
                                    const tgt = baseRightHandY - POSE_CONFIG.love.handYawIn;
                                    let out = def * (1 - w) + tgt * w;
                                    if (wt > 0 && !POSE_CONFIG.think.expressionsOnly) {
                                        const tHandYaw = baseRightHandY - POSE_CONFIG.think.handYawInExtra;
                                        out = out * (1 - wt) + tHandYaw * wt;
                                    }
                                    rightHand.rotation.y = out;
                                }
                                
                                if (spineBone) {
                                    spineBone.position.y = baseSpineY + breathe;
                                }

                                // Head/neck tracking toward look target
                                if (vrmCamera && lookTarget && (neckBone || headBone)) {
                                    const camInv = new window.THREE.Matrix4().copy(vrmCamera.matrixWorld).invert();
                                    const tgtCam = new window.THREE.Vector3().copy(lookTarget.position).applyMatrix4(camInv);
                                    const yaw = Math.atan2(tgtCam.x, tgtCam.z);
                                    const pitch = Math.atan2(-tgtCam.y, tgtCam.z);
                                    const maxYaw = 0.35;
                                    const maxPitch = 0.25;
                                    const clampedYaw = Math.max(-maxYaw, Math.min(maxYaw, yaw));
                                    const clampedPitch = Math.max(-maxPitch, Math.min(maxPitch, pitch));
                                    const neckWeight = 0.4;
                                    const headWeight = 0.6;
                                    const smoothing = 0.12;
                                    
                                    if (neckBone) {
                                        const targetNeckY = baseNeckY + clampedYaw * neckWeight;
                                        const targetNeckX = baseNeckX + clampedPitch * neckWeight;
                                        neckBone.rotation.y += (targetNeckY - neckBone.rotation.y) * smoothing;
                                        neckBone.rotation.x += (targetNeckX - neckBone.rotation.x) * smoothing;
                                    }
                                    
                                    if (headBone) {
                                        const targetHeadY = baseHeadY + clampedYaw * headWeight;
                                        const targetHeadX = baseHeadX + clampedPitch * headWeight;
                                        headBone.rotation.y += (targetHeadY - headBone.rotation.y) * smoothing;
                                        headBone.rotation.x += (targetHeadX - headBone.rotation.x) * smoothing;
                                    }
                                }
                            }
                        }
                    }

                    if (vrmRenderer && vrmScene && vrmCamera) {
                        vrmRenderer.render(vrmScene, vrmCamera);
                    }
                }
                animate();

                // Start periodic blinking after VRM is ready
                const setBlink = (value) => {
                    if (!vrmModel) return;
                    try { if (vrmModel.expressionManager) { vrmModel.expressionManager.setValue('blink', value); } } catch (_) {}
                    try { if (vrmModel.expressionManager) { vrmModel.expressionManager.setValue('blinkLeft', value); } } catch (_) {}
                    try { if (vrmModel.expressionManager) { vrmModel.expressionManager.setValue('blinkRight', value); } } catch (_) {}
                    try { if (vrmModel.blendShapeProxy) { vrmModel.blendShapeProxy.setValue('Blink', value); } } catch (_) {}
                    try { if (vrmModel.blendShapeProxy) { vrmModel.blendShapeProxy.setValue('Blink_L', value); } } catch (_) {}
                    try { if (vrmModel.blendShapeProxy) { vrmModel.blendShapeProxy.setValue('Blink_R', value); } } catch (_) {}
                };

                const scheduleBlink = () => {
                    if (!vrmModel) return;
                    const waitMs = 2200 + Math.random() * 2600;
                    vrmBlinkTimeout = setTimeout(() => {
                        setBlink(1.0);
                        setTimeout(() => {
                            setBlink(0.0);
                            scheduleBlink();
                        }, 120 + Math.random() * 80);
                    }, waitMs);
                };
                scheduleBlink();

                console.log('VRM model loaded successfully');
            } catch (error) {
                console.error('Failed to load VRM model:', error);
            }
        }

        // Load avatar mode preference and initialize
        (async () => {
            try {
                const savedMode = storage.getItem('avatarMode') || getSetting('avatarMode', 'live2d');
                // Delay avatar initialization to ensure DOM and modules are ready
                setTimeout(async () => {
                    try {
                        if (savedMode === 'live2d') {
                            await switchToLive2D();
                        } else {
                            await switchToVRM();
                        }
                    } catch (e) {
                        console.warn('Error initializing avatar:', e);
                        try {
                            await switchToLive2D();
                        } catch (e2) {
                            console.error('Failed to initialize Live2D as fallback:', e2);
                        }
                    }
                }, 500);
            } catch (e) {
                console.warn('Error loading avatar mode preference:', e);
            }
        })();

        // ============================================================================
        // SPEECH TO TEXT (STT)
        // ============================================================================
        
        let audioContext;
        let mediaStreamSource;
        let recorderNode;
        let audioData = [];
        let isRecording = false;

        async function initAudioRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                mediaStreamSource = audioContext.createMediaStreamSource(stream);

                await audioContext.audioWorklet.addModule('recorder-worklet-processor.js');
                recorderNode = new AudioWorkletNode(audioContext, 'recorder-worklet');
                recorderNode.port.onmessage = (event) => {
                    const inputData = event.data;
                    if (audioData.length % 100 === 0) {
                        console.log('Received audio data chunk:', inputData.length, 'samples');
                    }
                    audioData.push(new Float32Array(inputData));
                };

                sttBtn.innerHTML = '<i class="fas fa-microphone"></i>';
                sttBtn.title = "Start Recording";
                console.log('Audio recording initialized successfully');
            } catch (error) {
                console.error('Unable to access microphone:', error);
                showStatus('Unable to access microphone: ' + error.message, 5000);
            }
        }

        function toggleRecording() {
            if (!isRecording) {
                startRecording();
            } else {
                stopRecording();
            }
        }

        function startRecording() {
            if (!isRecording && recorderNode && audioContext) {
                speechSynthesis.cancel();
                
                if (live2dModel) {
                    live2dModel.expression(null);
                    if (live2dModel.internalModel) {
                        live2dModel.internalModel.coreModel.setParameterValueById('ParamAngleX', 0);
                        live2dModel.internalModel.coreModel.setParameterValueById('ParamAngleY', 0);
                        live2dModel.internalModel.coreModel.setParameterValueById('ParamAngleZ', 0);
                        live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0);
                    }
                }

                audioData = [];
                userInput.value = '';

                mediaStreamSource.connect(recorderNode);
                
                sttBtn.innerHTML = '<i class="fas fa-stop"></i>';
                sttBtn.title = "Stop Recording";
                sttBtn.classList.add('recording');
                showStatus("Recording...", 1000);
                isRecording = true;
                console.log("Recording started");
            }
        }

        function stopRecording() {
            if (!isRecording || !recorderNode || !audioContext) {
                isRecording = false;
                sttBtn.innerHTML = '<i class="fas fa-microphone"></i>';
                sttBtn.title = "Start Recording";
                sttBtn.classList.remove('recording');
                return;
            }
            
            try {
                if (mediaStreamSource) {
                    try {
                        mediaStreamSource.disconnect();
                    } catch (disconnectError) {
                        console.warn('MediaStreamSource disconnect warning:', disconnectError.message);
                    }
                }
                
                if (recorderNode) {
                    try {
                        recorderNode.disconnect();
                    } catch (disconnectError) {
                        console.warn('RecorderNode disconnect warning:', disconnectError.message);
                    }
                }
            } catch (error) {
                console.error('Error during audio node disconnection:', error);
            }
            
            sttBtn.innerHTML = '<i class="fas fa-microphone"></i>';
            sttBtn.title = "Start Recording";
            sttBtn.classList.remove('recording');
            showStatus("Processing recording...", 2000);
            
            isRecording = false;
            
            if (audioData.length > 0) {
                processAudioData();
            } else {
                showStatus("No audio recorded", 2000);
            }
        }

        function processAudioData() {
            if (!audioContext || !audioContext.sampleRate) {
                console.error('Audio context is not available for processing');
                showStatus("Processing failed: Audio context unavailable.", 3000);
                audioData = [];
                return;
            }
            
            let flatData = flattenArray(audioData);
            
            console.log('Audio data chunks:', audioData.length);
            console.log('Total samples:', flatData.length);
            console.log('Sample rate:', audioContext.sampleRate);
            const durationSeconds = flatData.length / audioContext.sampleRate;
            console.log('Recording duration:', durationSeconds.toFixed(2), 'seconds');
            
            if (durationSeconds < 0.5) {
                console.error('Recording is too short:', durationSeconds, 'seconds');
                showStatus("Recording too short. Please speak for at least 0.5 seconds.", 3000);
                audioData = [];
                return;
            }
            
            let maxAmplitude = 0;
            let sumAmplitude = 0;
            for (let i = 0; i < flatData.length; i++) {
                const absValue = Math.abs(flatData[i]);
                maxAmplitude = Math.max(maxAmplitude, absValue);
                sumAmplitude += absValue;
            }
            const avgAmplitude = sumAmplitude / flatData.length;
            console.log('Max amplitude:', maxAmplitude.toFixed(4));
            console.log('Average amplitude:', avgAmplitude.toFixed(4));
            
            if (maxAmplitude < 0.01) {
                console.warn('Audio appears to be very quiet or silent');
                showStatus("Audio too quiet. Please speak louder or check your microphone.", 3000);
                audioData = [];
                return;
            }

            let wavBlob = encodeWAV(flatData, audioContext.sampleRate);
            console.log('WAV blob size:', wavBlob.size, 'bytes');
            
            audioData = [];
            sendAudioToWhisper(wavBlob);
        }

        function encodeWAV(samples, sampleRate) {
            const buffer = new ArrayBuffer(44 + samples.length * 2);
            const view = new DataView(buffer);

            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + samples.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, samples.length * 2, true);

            floatTo16BitPCM(view, 44, samples);

            return new Blob([view], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

        function floatTo16BitPCM(output, offset, input) {
            for (let i = 0; i < input.length; i++, offset += 2) {
                let s = Math.max(-1, Math.min(1, input[i]));
                output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
        }

        function flattenArray(channelData) {
            let length = channelData.reduce((acc, val) => acc + val.length, 0);
            let result = new Float32Array(length);
            let offset = 0;
            for (let data of channelData) {
                result.set(data, offset);
                offset += data.length;
            }
            return result;
        }

        async function sendAudioToWhisper(audioBlob) {
            const apiKey = getApiKey();
            if (!apiKey || apiKey.length === 0) {
                console.error('API key is required for transcription');
                showStatus("Transcription failed: API key is missing.", 3000);
                return;
            }
            
            const whisperEndpoint = `${PROXY_BASE_URL}/v1/audio/transcriptions`;

            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.wav');
            formData.append('model', 'whisper-1');

            try {
                console.log('Sending audio to Whisper...');
                const response = await fetch(whisperEndpoint, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: formData
                });
                
                if (!response.ok) {
                    let errorMessage = `Transcription failed: Server returned ${response.status} ${response.statusText}`;
                    try {
                        const errorData = await response.json();
                        if (errorData.error && errorData.error.message) {
                            errorMessage = `Transcription failed: ${errorData.error.message}`;
                        } else if (errorData.message) {
                            errorMessage = `Transcription failed: ${errorData.message}`;
                        }
                    } catch (parseError) {
                        if (response.status === 401) {
                            errorMessage = "Transcription failed: Invalid or missing API key.";
                        } else if (response.status === 404) {
                            errorMessage = "Transcription failed: Endpoint not found.";
                        } else if (response.status >= 500) {
                            errorMessage = "Transcription failed: Server error.";
                        }
                    }
                    console.error('Whisper API error:', errorMessage);
                    showStatus(errorMessage, 5000);
                    return;
                }
                
                let responseText;
                let data;
                try {
                    responseText = await response.text();
                    data = JSON.parse(responseText);
                } catch (jsonError) {
                    console.error('Failed to parse Whisper response as JSON:', jsonError);
                    showStatus("Transcription failed: Invalid server response format.", 3000);
                    return;
                }
                
                console.log('Whisper response:', data);
                
                let transcribedText = null;
                if (data.text) {
                    transcribedText = data.text;
                } else if (data.transcription) {
                    transcribedText = data.transcription;
                } else if (typeof data === 'string') {
                    transcribedText = data;
                } else if (data.result && data.result.text) {
                    transcribedText = data.result.text;
                }
                
                if (transcribedText) {
                    transcribedText = transcribedText.trim();
                    console.log('Transcribed text extracted:', transcribedText);
                    
                    if (!transcribedText || transcribedText.length === 0) {
                        console.error('Transcribed text is empty after trimming');
                        showStatus("Transcription failed: No text was transcribed.", 3000);
                        return;
                    }
                    
                    userInput.value = transcribedText + ' ';
                    showStatus("Transcription successful.", 2000);
                    console.log('Calling fetchOpenAIResponse with transcribed text:', transcribedText);
                    fetchOpenAIResponse(transcribedText);
                    userInput.value = '';
                } else {
                    console.error('Unexpected response format - no text field found:', data);
                    showStatus("Transcription failed: Unexpected response format.", 3000);
                }
            } catch (error) {
                let errorMessage = "Transcription failed. Please try again.";
                if (error instanceof TypeError && error.message.includes('fetch')) {
                    errorMessage = "Transcription failed: Unable to connect to server.";
                } else if (error instanceof SyntaxError) {
                    errorMessage = "Transcription failed: Invalid server response.";
                } else {
                    errorMessage = `Transcription failed: ${error.message || 'Unknown error'}.`;
                }
                console.error('Error with OpenAI Whisper request:', error);
                showStatus(errorMessage, 5000);
            }
        }

        sttBtn.addEventListener('click', async function() {
            await resumeAudioContextOnce();
            if (!recorderNode) {
                await initAudioRecording();
            }
            toggleRecording();
        });

        // ============================================================================
        // PHILOSOPHER MODE
        // ============================================================================
        
        let philosopherModeActive = false;
        let philosopherModeStarting = false;
        let philosopherModeContemplating = false;
        let philosopherModeInterval = null;

        function detectPhilosopherModeTrigger(message) {
            const triggerPhrases = [
                'philosopher mode',
                'enable philosopher mode',
                'start philosopher mode',
                'enter philosopher mode',
                'philosopher',
            ];
            const messageLower = message.toLowerCase().trim();
            return triggerPhrases.some(phrase => messageLower.includes(phrase));
        }

        async function startPhilosopherMode() {
            if (philosopherModeActive || philosopherModeStarting) {
                return;
            }

            philosopherModeStarting = true;

            try {
                const response = await fetch(`${PROXY_BASE_URL}/v1/philosopher/start`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        conversation_id: activeConversationId || 'default',
                        user_id: activeConversationId || 'default',
                    }),
                });

                if (!response.ok) {
                    let errorMessage = 'Unknown error';
                    try {
                        const errorData = await response.json();
                        errorMessage = errorData.detail || errorData.message || `HTTP ${response.status}: ${response.statusText}`;
                    } catch (e) {
                        errorMessage = `HTTP ${response.status}: ${response.statusText}`;
                    }
                    console.error('Failed to start philosopher mode:', errorMessage);
                    showStatus(`Failed to start philosopher mode: ${errorMessage}`, 5000);
                    return;
                }

                const data = await response.json();
                if (!philosopherModeStarting) {
                    return;
                }

                if (data.success === true) {
                    if (!philosopherModeStarting) {
                        return;
                    }
                    if (!philosopherModeStarting) {
                        return;
                    }
                    philosopherModeActive = true;
                    if (!philosopherModeStarting) {
                        philosopherModeActive = false;
                        return;
                    }
                    if (philosopherBtn) {
                        philosopherBtn.classList.add('philosopher-active');
                    }
                    addMessageToHistory('assistant', 'Philosopher Mode activated. I will now contemplate questions and share my thoughts.');
                    startPhilosopherContemplationLoop();
                } else {
                    console.error('Failed to start philosopher mode:', data.message);
                    showStatus(`Failed to start philosopher mode: ${data.message || 'Unknown error'}`, 5000);
                }
            } catch (error) {
                console.error('Error starting philosopher mode:', error);
                showStatus(`Error starting philosopher mode: ${error.message}`, 5000);
            } finally {
                philosopherModeStarting = false;
            }
        }

        async function stopPhilosopherMode(skipMessage = false) {
            if (!philosopherModeActive && !philosopherModeStarting) {
                return;
            }

            try {
                philosopherModeActive = false;
                if (philosopherModeInterval) {
                    clearInterval(philosopherModeInterval);
                    philosopherModeInterval = null;
                }

                const wasStarting = philosopherModeStarting;
                if (philosopherModeStarting) {
                    philosopherModeStarting = false;
                }

                const response = await fetch(`${PROXY_BASE_URL}/v1/philosopher/stop`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        conversation_id: activeConversationId || 'default',
                        user_id: activeConversationId || 'default',
                    }),
                });

                const data = await response.json();
                philosopherModeActive = false;
                philosopherModeContemplating = false;
                if (philosopherBtn) {
                    philosopherBtn.classList.remove('philosopher-active');
                }
                
                if (!skipMessage) {
                    if (wasStarting) {
                        addMessageToHistory('assistant', 'Philosopher Mode activation cancelled.');
                    } else {
                        addMessageToHistory('assistant', 'Philosopher Mode deactivated.');
                    }
                }
            } catch (error) {
                console.error('Error stopping philosopher mode:', error);
                philosopherModeActive = false;
                philosopherModeContemplating = false;
                philosopherModeStarting = false;
                if (philosopherBtn) {
                    philosopherBtn.classList.remove('philosopher-active');
                }
            }
        }

        async function executeContemplation() {
            if (!philosopherModeActive || philosopherModeContemplating) {
                return;
            }

            philosopherModeContemplating = true;

            try {
                const response = await fetch(`${PROXY_BASE_URL}/v1/philosopher/contemplate`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        conversation_id: activeConversationId || 'default',
                        user_id: activeConversationId || 'default',
                    }),
                });

                if (!response.ok) {
                    let errorMessage = 'Unknown error';
                    try {
                        const errorData = await response.json();
                        errorMessage = errorData.detail || errorData.message || `HTTP ${response.status}: ${response.statusText}`;
                    } catch (e) {
                        errorMessage = `HTTP ${response.status}: ${response.statusText}`;
                    }
                    console.error('Contemplation failed:', errorMessage);
                    if (philosopherModeActive) {
                        addMessageToHistory('assistant', `⚠️ Contemplation error: ${errorMessage}`);
                    }
                    return;
                }

                const data = await response.json();
                if (!philosopherModeActive) {
                    return;
                }
                
                if (data.success && data.data) {
                    if (!philosopherModeActive) {
                        return;
                    }
                    if (data.data.question) {
                        addMessageToHistory('philosopher', `🤔 Question: ${data.data.question}`);
                    }
                    if (!philosopherModeActive) {
                        return;
                    }
                    if (data.data.conclusion) {
                        addMessageToHistory('philosopher', `💭 Conclusion: ${data.data.conclusion}`);
                    }
                } else {
                    const errorMsg = data.message || 'Unknown error';
                    console.error('Contemplation failed:', errorMsg);
                    if (philosopherModeActive) {
                        addMessageToHistory('assistant', `⚠️ Contemplation failed: ${errorMsg}`);
                    }
                }
            } catch (error) {
                console.error('Error during contemplation:', error);
                if (philosopherModeActive) {
                    addMessageToHistory('assistant', `⚠️ Contemplation error: ${error.message || 'Network or connection error'}`);
                }
            } finally {
                philosopherModeContemplating = false;
            }
        }

        function startPhilosopherContemplationLoop() {
            if (philosopherModeInterval) {
                clearInterval(philosopherModeInterval);
            }

            executeContemplation();

            philosopherModeInterval = setInterval(() => {
                if (philosopherModeActive && !philosopherModeContemplating) {
                    executeContemplation();
                }
            }, 30000);
        }

        async function togglePhilosopherMode() {
            if (philosopherModeActive) {
                await stopPhilosopherMode();
            } else {
                await startPhilosopherMode();
            }
        }

        philosopherBtn.addEventListener('click', togglePhilosopherMode);

        // ============================================================================
        // CHAT API INTEGRATION
        // ============================================================================
        
        function getApiKey() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.apiKey || '';
            } catch {
                return '';
            }
        }

        function getEndpoint() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.endpoint || 'http://localhost:1234/v1/chat/completions';
            } catch {
                return 'http://localhost:1234/v1/chat/completions';
            }
        }

        function getSystemPrompt() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.systemPrompt || 'You are EVA, a helpful AI assistant.';
            } catch {
                return 'You are EVA, a helpful AI assistant.';
            }
        }

        function getCurrentModel() {
            try {
                const settings = JSON.parse(storage.getItem('toolSettings') || '{}');
                return settings.baseModel || '';
            } catch {
                return '';
            }
        }

        async function fetchOpenAIResponse(promptText) {
            if (!promptText || typeof promptText !== 'string') {
                console.error('fetchOpenAIResponse called with invalid promptText:', promptText);
                showStatus("Error: Invalid input text.", 3000);
                return;
            }
            
            promptText = promptText.trim();
            if (promptText.length === 0) {
                console.error('fetchOpenAIResponse called with empty promptText');
                showStatus("Error: Empty input text.", 3000);
                return;
            }
            
            console.log('fetchOpenAIResponse called with promptText:', promptText);
            
            let endpoint = getEndpoint();
            const apiKey = getApiKey();

            const systemPrompt = getSystemPrompt();
            
            const messages = [
                { 
                    role: 'system', 
                    content: systemPrompt
                },
                ...chatHistory,
                {
                    role: 'user',
                    content: promptText
                }
            ];

            const body = {
                model: getCurrentModel(),
                messages: messages,
                max_tokens: 4096,
                temperature: 0.7,
                stream: false
            };

            console.log('Adding user message to history with content:', promptText);
            addMessageToHistory('user', promptText);
            
            try {
                messageHistory.classList.add('responding');
                console.log('Sending request:', {
                    endpoint,
                    model: getCurrentModel(),
                    prompt: promptText
                });
                
                const response = await fetch(endpoint, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: JSON.stringify(body)
                });

                const data = await response.json();
                console.log('Response from LLM:', JSON.stringify(data, null, 2));

                if (data.choices && data.choices.length > 0) {
                    const message = data.choices[0].message;
                    console.log('Processing message:', message);

                    const content = message.content || '';
                    const finalContent = content.trim();
                    
                    if (finalContent) {
                        chatHistory.push({ role: 'assistant', content: finalContent });
                        addMessageToHistory('assistant', finalContent);
                        textToSpeech(finalContent);
                    }
                } else {
                    console.error('No choices in response:', data);
                    showStatus("Error: No response from AI.", 3000);
                }
            } catch (error) {
                console.error('Error with OpenAI request:', error);
                showStatus(`Error: ${error.message || 'Network error'}`, 5000);
            } finally {
                messageHistory.classList.remove('responding');
            }
        }

        // Text to Speech with lip sync support
        function removeEmojis(text) {
            const emojiRegex = /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F700}-\u{1F77F}\u{1F780}-\u{1F7FF}\u{1F800}-\u{1F8FF}\u{1F900}-\u{1F9FF}\u{1FA00}-\u{1FA6F}\u{1FA70}-\u{1FAFF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{2300}-\u{23FF}\u{2B50}\u{2B55}\u{231A}\u{231B}\u{2328}\u{23CF}\u{23E9}-\u{23FF}\u{24C2}\u{25AA}-\u{25AB}\u{25B6}\u{25C0}\u{25FB}-\u{25FE}\u{2934}-\u{2935}\u{2B05}-\u{2B07}\u{2B1B}-\u{2B1C}\u{3030}\u{303D}\u{3297}\u{3299}\u{FE0F}\u{200D}]/gu;
            return text.replace(emojiRegex, '');
        }

        function stripBracketedContent(text) {
            const patterns = [/\([^()]*\)/g, /\[[^\[\]]*\]/g, /\{[^{}]*\}/g, /<[^<>]*>/g];
            let prev = null;
            let curr = String(text);
            while (curr !== prev) {
                prev = curr;
                for (const re of patterns) { curr = curr.replace(re, ''); }
            }
            return curr;
        }

        function sanitizeTTS(text) {
            if (!text) return text;
            let t = String(text);
            t = removeEmojis(t);
            t = stripBracketedContent(t);
            t = t.replace(/\*/g, '');
            t = t.replace(/[^\w\s\.,!\?;:'"\-]/g, '');
            t = t.replace(/\s+/g, ' ').trim();
            return t;
        }

        function textToSpeech(text) {
            if (!text) {
                console.warn('No text provided for speech');
                return;
            }

            if (isMuted) {
                console.log('TTS is muted, skipping speech');
                return;
            }

            // Sanitize text
            text = sanitizeTTS(text);

            // Cancel any ongoing speech and active lip-sync loops
            try { speechSynthesis.cancel(); } catch (_) {}
            try { if (ttsLipSyncIntervalId) { clearInterval(ttsLipSyncIntervalId); ttsLipSyncIntervalId = null; } } catch (_) {}
            try { if (ttsRafId) { cancelAnimationFrame(ttsRafId); ttsRafId = 0; } } catch (_) {}
            try { ttsCleanupFns.forEach(fn => { try { fn(); } catch(_){} }); ttsCleanupFns = []; } catch (_) {}
            try { stopChatterboxLipSync(true); } catch (_) {}
            try { if (ttsAbortController) { ttsAbortController.abort(); ttsAbortController = null; } } catch (_) {}
            // Stop Microsoft TTS lip sync if active
            try { 
                microsoftTtsIsActive = false;
                if (microsoftTtsRafId) { 
                    cancelAnimationFrame(microsoftTtsRafId); 
                    microsoftTtsRafId = 0; 
                }
            } catch (_) {}

            // Check TTS service selection
            const ttsServiceMicrosoft = document.getElementById('settings-tts-service-microsoft');
            const ttsServiceOpenAI = document.getElementById('settings-tts-service-openai');
            const useOpenAITTS = (ttsServiceOpenAI && ttsServiceOpenAI.checked) || 
                                 (getSetting('ttsService', 'microsoft') === 'openai');
            
            if (useOpenAITTS) {
                speakWithOpenAITTS(text);
                return;
            }

            // Microsoft TTS path
            const utterance = new SpeechSynthesisUtterance(text);
            const microsoftVoiceDropdown = document.getElementById('settings-microsoft-voice');
            
            // Get voices if not already loaded
            if (voices.length === 0) {
                voices = speechSynthesis.getVoices();
            }
            
            // Select voice from dropdown
            if (microsoftVoiceDropdown && microsoftVoiceDropdown.value !== '') {
                const selectedVoiceIndex = parseInt(microsoftVoiceDropdown.value);
                if (!isNaN(selectedVoiceIndex) && voices[selectedVoiceIndex]) {
                    utterance.voice = voices[selectedVoiceIndex];
                    console.log('Using voice:', voices[selectedVoiceIndex].name);
                    // Persist selection
                    try {
                        storage.setItem(SELECTED_VOICE_STORAGE_KEY, voices[selectedVoiceIndex].voiceURI);
                    } catch (e) {
                        console.warn('Could not save voice selection:', e);
                    }
                }
            } else if (voices.length > 0) {
                // Fallback to first available voice
                utterance.voice = voices[0];
                console.log('Using default voice:', voices[0].name);
            }

            const mouthOpenY = "ParamMouthOpenY";
            
            // Start smooth amplitude-based lip sync for Microsoft TTS
            function startMicrosoftTtsLipSync() {
                if (microsoftTtsRafId) return;
                
                microsoftTtsIsActive = true;
                microsoftTtsSmoothedAmplitude = 0;
                microsoftTtsTargetAmplitude = 0.3;
                microsoftTtsLastBoundaryTs = performance.now();
                
                const attack = 0.6;
                const release = 0.15;
                const threshold = 0.05;
                const boundaryDecayRate = 0.08;
                const boundaryDecayDelay = 50;
                
                const step = () => {
                    if (!microsoftTtsIsActive || !isSpeaking) {
                        microsoftTtsRafId = 0;
                        microsoftTtsSmoothedAmplitude = 0;
                        if (live2dModel) {
                            live2dModel.internalModel.coreModel.setParameterValueById(mouthOpenY, 0);
                        }
                        if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                            animateVRMLipSync(0);
                        }
                        return;
                    }
                    
                    const now = performance.now();
                    const timeSinceBoundary = now - microsoftTtsLastBoundaryTs;
                    
                    if (timeSinceBoundary > boundaryDecayDelay) {
                        const decayFactor = Math.min(1.0, (timeSinceBoundary - boundaryDecayDelay) / 200);
                        microsoftTtsTargetAmplitude = Math.max(0, microsoftTtsTargetAmplitude - (boundaryDecayRate * decayFactor));
                    }
                    
                    if (microsoftTtsTargetAmplitude > microsoftTtsSmoothedAmplitude) {
                        microsoftTtsSmoothedAmplitude += (microsoftTtsTargetAmplitude - microsoftTtsSmoothedAmplitude) * attack;
                    } else {
                        microsoftTtsSmoothedAmplitude += (microsoftTtsTargetAmplitude - microsoftTtsSmoothedAmplitude) * release;
                    }
                    
                    const scaled = microsoftTtsSmoothedAmplitude <= threshold ? 0 : Math.min(1, (microsoftTtsSmoothedAmplitude - threshold) * 5.5);
                    
                    if (live2dModel) {
                        live2dModel.internalModel.coreModel.setParameterValueById(mouthOpenY, scaled);
                    }
                    
                    if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                        animateVRMLipSync(scaled);
                    }
                    
                    if (isSpeaking && microsoftTtsIsActive) {
                        microsoftTtsRafId = requestAnimationFrame(step);
                    } else {
                        microsoftTtsRafId = 0;
                    }
                };
                
                step();
            }
            
            utterance.onstart = function() {
                console.log('Speech started');
                isSpeaking = true;
                microsoftTtsLastBoundaryTs = performance.now();
                startMicrosoftTtsLipSync();
                if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                    animateVRMLipSync(0.8);
                }
            };

            utterance.onboundary = function(event) {
                const now = performance.now();
                const timeSinceLastBoundary = now - microsoftTtsLastBoundaryTs;
                microsoftTtsLastBoundaryTs = now;
                const amplitudeBoost = Math.min(0.35, 0.15 + (timeSinceLastBoundary / 1000) * 0.08);
                microsoftTtsTargetAmplitude = Math.min(0.85, microsoftTtsTargetAmplitude + amplitudeBoost);
            };

            utterance.onend = function() {
                console.log('Speech ended');
                isSpeaking = false;
                microsoftTtsIsActive = false;
                if (microsoftTtsRafId) {
                    try { cancelAnimationFrame(microsoftTtsRafId); } catch(_){}
                    microsoftTtsRafId = 0;
                }
                if (ttsLipSyncIntervalId) { try { clearInterval(ttsLipSyncIntervalId); } catch(_){} ttsLipSyncIntervalId = null; }
                if (ttsRafId) { try { cancelAnimationFrame(ttsRafId); } catch(_){} ttsRafId = 0; }
                try { ttsCleanupFns.forEach(fn => { try { fn(); } catch(_){} }); } catch(_){}
                ttsCleanupFns = [];

                if (live2dModel) {
                    live2dModel.internalModel.coreModel.setParameterValueById(mouthOpenY, 0);
                }
                if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                    animateVRMLipSync(0.0);
                }
            };

            utterance.onerror = function(event) {
                console.error('Speech synthesis error:', event);
            };

            utterance.rate = 1.0;
            utterance.pitch = 1.0;

            try {
                speechSynthesis.speak(utterance);
            } catch (error) {
                console.error('Speech synthesis error:', error);
            }
        }

        // Lip sync functions for OpenAI TTS
        let ttsAnalyserGainNode = null;
        let ttsAnalyserDataArray = null;
        let ttsAnalyserStopTimer = null;
        let ttsPcmActiveSources = 0;

        function ensureChatterboxLipSyncGraph() {
            if (!window.__opus) { window.__opus = {}; }
            let ctx = window.__opus.audioCtx;
            if (!ctx) {
                ctx = new (window.AudioContext || window.webkitAudioContext)();
                window.__opus.audioCtx = ctx;
                window.__opus.playhead = 0;
            }

            if (!ttsAnalyserNode || ttsAnalyserNode.context !== ctx) {
                try { if (ttsAnalyserNode) ttsAnalyserNode.disconnect(); } catch (_) {}
                try { if (ttsAnalyserGainNode) ttsAnalyserGainNode.disconnect(); } catch (_) {}
                ttsAnalyserNode = ctx.createAnalyser();
                ttsAnalyserNode.fftSize = 1024;
                ttsAnalyserNode.smoothingTimeConstant = 0.7;
                ttsAnalyserGainNode = ctx.createGain();
                ttsAnalyserGainNode.gain.value = 1.0;
                ttsAnalyserNode.connect(ttsAnalyserGainNode);
                ttsAnalyserGainNode.connect(ctx.destination);
            }

            if (!ttsAnalyserDataArray || ttsAnalyserDataArray.length !== ttsAnalyserNode.fftSize) {
                ttsAnalyserDataArray = new Uint8Array(ttsAnalyserNode.fftSize);
            }

            if (ttsAnalyserStopTimer) {
                clearTimeout(ttsAnalyserStopTimer);
                ttsAnalyserStopTimer = null;
            }

            return window.__opus.audioCtx;
        }

        function startLipSyncFromAnalyserNode() {
            try {
                if (!ttsAnalyserNode) return;

                if (!ttsAnalyserDataArray || ttsAnalyserDataArray.length !== ttsAnalyserNode.fftSize) {
                    ttsAnalyserDataArray = new Uint8Array(ttsAnalyserNode.fftSize);
                }

                if (ttsAnalyserLoopActive) return;

                let smoothed = 0;
                const attack = 0.6;
                const release = 0.15;
                const threshold = 0.03;

                const step = () => {
                    if (!ttsAnalyserNode) {
                        ttsAnalyserLoopActive = false;
                        ttsRafId = 0;
                        return;
                    }

                    ttsAnalyserNode.getByteTimeDomainData(ttsAnalyserDataArray);
                    let sum = 0;
                    for (let i = 0; i < ttsAnalyserDataArray.length; i++) {
                        const v = (ttsAnalyserDataArray[i] - 128) / 128;
                        sum += v * v;
                    }

                    const rms = Math.sqrt(sum / ttsAnalyserDataArray.length);
                    if (rms > smoothed) {
                        smoothed += (rms - smoothed) * attack;
                    } else {
                        smoothed += (rms - smoothed) * release;
                    }

                    const scaled = smoothed <= threshold ? 0 : Math.min(1, (smoothed - threshold) * 6.0);

                    if (live2dModel) {
                        live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', scaled);
                    }

                    if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                        animateVRMLipSync(scaled);
                    }

                    if (ttsStreamActive || ttsPcmActiveSources > 0 || smoothed > 0.0005) {
                        ttsRafId = requestAnimationFrame(step);
                    } else {
                        ttsRafId = 0;
                        ttsAnalyserLoopActive = false;
                        if (live2dModel) {
                            live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0);
                        }
                        if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                            animateVRMLipSync(0);
                        }
                    }
                };

                ttsAnalyserLoopActive = true;
                step();
            } catch (e) {
                console.warn('startLipSyncFromAnalyserNode failed:', e);
            }
        }

        function stopChatterboxLipSync(immediate = false) {
            if (immediate) {
                ttsStreamActive = false;
                ttsPcmActiveSources = 0;
                try { if (window.__opus && typeof window.__opus.playhead !== 'undefined') { window.__opus.playhead = 0; } } catch(_) {}
                if (ttsAnalyserStopTimer) {
                    clearTimeout(ttsAnalyserStopTimer);
                    ttsAnalyserStopTimer = null;
                }
                if (ttsRafId) {
                    try { cancelAnimationFrame(ttsRafId); } catch (_) {}
                    ttsRafId = 0;
                }
                ttsAnalyserLoopActive = false;
                if (live2dModel) {
                    live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0);
                }
                if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') {
                    animateVRMLipSync(0);
                }
                return;
            }

            if (ttsAnalyserStopTimer) {
                clearTimeout(ttsAnalyserStopTimer);
            }

            ttsAnalyserStopTimer = setTimeout(() => {
                if (ttsStreamActive || ttsPcmActiveSources > 0) {
                    return;
                }
                stopChatterboxLipSync(true);
            }, 250);
        }

        // Parse Server-Sent Events stream for TTS
        async function streamSSE(response, { onInit, onDelta, onDone, onError, onInfo }) {
            const reader = response.body.getReader(); // Get stream reader
            const decoder = new TextDecoder(); // Create text decoder that tolerates chunk splits
            let buf = ''; // Buffer for incomplete SSE events
            let chunkCount = 0; // Debug: count chunks received
            
            console.log('📡 Starting SSE stream parsing'); // Debug log
            
            try { // Guard SSE parsing
                // Check first chunk to see if it's actually binary audio data
                const firstChunk = await reader.read();
                if (firstChunk.done) {
                    console.log('📡 SSE stream ended immediately (empty)');
                    onDone?.();
                    return;
                }
                
                // Check if first chunk is binary audio (RIFF header for WAV, or other audio formats)
                const firstBytes = new Uint8Array(firstChunk.value);
                const isBinaryAudio = firstBytes.length >= 4 && (
                    (firstBytes[0] === 0x52 && firstBytes[1] === 0x49 && firstBytes[2] === 0x46 && firstBytes[3] === 0x46) || // RIFF (WAV)
                    (firstBytes[0] === 0xFF && firstBytes[1] === 0xFB) || // MP3
                    (firstBytes[0] === 0x4F && firstBytes[1] === 0x67 && firstBytes[2] === 0x67 && firstBytes[3] === 0x53) || // OggS
                    (firstBytes[0] === 0x1A && firstBytes[1] === 0x45 && firstBytes[2] === 0xDF && firstBytes[3] === 0xA3) // WebM/EBML
                );
                
                if (isBinaryAudio) {
                    console.log('📡 Detected binary audio in SSE stream, collecting all chunks');
                    // This is actually binary audio, not SSE - collect all chunks
                    const audioChunks = [firstChunk.value];
                    while (true) {
                        const { value, done } = await reader.read();
                        if (done) break;
                        audioChunks.push(value);
                    }
                    // Create blob from all chunks
                    const audioBlob = new Blob(audioChunks);
                    console.log('📡 Collected binary audio blob, size:', audioBlob.size, 'bytes');
                    // Attach blob to error so catch handler can use it
                    const error = new Error('BINARY_AUDIO_IN_SSE_STREAM');
                    error.audioBlob = audioBlob; // Attach blob to error
                    throw error;
                }
                
                // It's actually text/SSE, process normally
                chunkCount = 1;
                const decoded = decoder.decode(firstChunk.value, { stream: true });
                console.log('📡 Received SSE chunk #' + chunkCount + ', length:', decoded.length, 'preview:', decoded.substring(0, 200)); // Debug log
                buf += decoded; // Add to buffer
                
                while (true) { // Loop until stream ends
                    const { value, done } = await reader.read(); // Read next chunk
                    if (done) {
                        console.log('📡 SSE stream ended, total chunks:', chunkCount, 'buffer length:', buf.length); // Debug log
                        break; // Exit when stream completes
                    }
                    chunkCount++; // Increment chunk counter
                    const decoded = decoder.decode(value, { stream: true }); // Decode partial chunk
                    console.log('📡 Received SSE chunk #' + chunkCount + ', length:', decoded.length, 'preview:', decoded.substring(0, 200)); // Debug log
                    buf += decoded; // Add to buffer
                    
                    // SSE events end with a blank line (\n\n)
                    let idx; // Index of double newline
                    let eventCount = 0; // Debug: count events found
                    while ((idx = buf.indexOf('\n\n')) !== -1) { // Find complete SSE event
                        eventCount++; // Increment event counter
                        const rawEvent = buf.slice(0, idx); // Extract complete event
                        console.log('📡 Found SSE event #' + eventCount + ', length:', rawEvent.length, 'content:', rawEvent.substring(0, 300)); // Debug log
                        buf = buf.slice(idx + 2); // Remove processed event from buffer
                        
                        // Ignore comments/heartbeats starting with ":" lines
                        const lines = rawEvent.split(/\r?\n/).filter(l => !l.startsWith(':')); // Split by newline and filter comments
                        
                        // Concatenate multiple data: lines (valid SSE format)
                        let eventName = null; // Event type from event: line
                        const dataParts = []; // Array to collect data: lines
                        for (const line of lines) { // Process each line of the event
                            if (line.startsWith('event:')) { // Check for event type declaration
                                eventName = line.slice(6).trim(); // Extract event name
                            } else if (line.startsWith('data:')) { // Check for data line
                                dataParts.push(line.slice(5).trim()); // Extract data (remove 'data:' prefix)
                            } // End data line check
                        } // End line processing loop
                        
                        if (dataParts.length === 0) continue; // Skip events with no data
                        const dataStr = dataParts.join('\n'); // Join multiple data lines
                        if (dataStr === '[DONE]') { // Check for OpenAI-style completion
                            onDone?.(); // Call completion handler
                            return; // Exit parser
                        } // End [DONE] check
                        
                        // Expect JSON object per Chatterbox docs
                        let evt; // Parsed event object
                        try { // Guard JSON parsing
                            evt = JSON.parse(dataStr); // Parse JSON from data string
                        } catch (parseError) { // Catch JSON parse errors
                            console.warn('⚠️ Failed to parse SSE JSON:', parseError, 'Data:', dataStr.substring(0, 100)); // Debug log
                            continue; // Skip invalid JSON events
                        } // End JSON parse try/catch
                        
                        // Debug: Log all events to understand the format
                        console.log('🎵 SSE event received, type:', evt?.type, 'has audio:', !!evt?.audio, 'keys:', Object.keys(evt || {})); // Debug log
                        
                        // OpenAI-compatible naming from Chatterbox docs:
                        //   speech.audio.info   (metadata: sample_rate, channels, bits_per_sample)
                        //   speech.audio.init   (base64 WebM init segment)
                        //   speech.audio.delta  (base64 in evt.audio)
                        //   speech.audio.done
                        if (evt?.type === 'speech.audio.info') { // Check for metadata/info event
                            console.log('🎵 Received audio info:', evt); // Log metadata
                            onInfo?.(evt); // Pass metadata to handler
                        } else if (evt?.type === 'speech.audio.init' && evt?.audio) { // Check for init segment event
                            console.log('🎵 Received init event, size:', evt.audio.length); // Log init event
                            onInit?.(evt.audio); // Pass base64 init segment to handler
                        } else if (evt?.type === 'speech.audio.delta' && evt?.audio) { // Check for audio delta event
                            console.log('🎵 Received delta event, audio length:', evt.audio.length); // Debug log
                            onDelta?.(evt.audio); // Pass base64 string to handler
                        } else if (evt?.type === 'speech.audio.done') { // Check for completion event
                            console.log('🎵 Received done event'); // Log completion
                            onDone?.(evt.usage); // Call completion handler with usage info
                            return; // Exit parser
                        } else { // Unknown event type
                            console.warn('🎵 Unknown SSE event type:', evt?.type, 'Full event:', evt); // Log unknown event
                        } // End event type check
                    } // End event parsing loop
                } // End stream reading loop
                onDone?.(); // Call completion handler when stream ends
            } catch (e) { // Catch SSE parsing errors
                onError?.(e); // Call error handler
            } // End try/catch
        } // End streamSSE

        // Play PCM16 delta chunk using Web Audio API
        function playPcm16Delta(base64, sampleRate = 24000, channels = 1) {
            let bin; // Declare bin variable
            try {
                bin = atob(base64); // Decode base64 to binary string
            } catch (decodeError) {
                console.error('❌ Failed to decode base64 PCM16 chunk:', decodeError);
                return;
            }
            
            const totalSamples = bin.length / 2; // Calculate total number of int16 samples across all channels (2 bytes/sample)
            if (!Number.isFinite(totalSamples) || totalSamples <= 0) {
                console.warn('⚠️ Received empty PCM16 chunk, bin length:', bin.length);
                return;
            }
            console.log('🔊 Decoding PCM16 chunk:', totalSamples, 'samples,', sampleRate, 'Hz,', channels, 'channels'); // Debug log

            const i16 = new Int16Array(totalSamples); // Create Int16Array for signed 16-bit samples
            
            // Convert little-endian bytes to int16 samples
            for (let i = 0, o = 0; i < totalSamples; i++, o += 2) { // Loop through samples (2 bytes each)
                // little-endian 16-bit signed conversion
                i16[i] = (bin.charCodeAt(o) | (bin.charCodeAt(o + 1) << 8)); // Combine bytes (little-endian)
            } // End byte conversion loop

            // Convert Int16 to Float32 [-1, 1] range
            const f32 = new Float32Array(totalSamples); // Create Float32Array for normalized samples
            for (let i = 0; i < totalSamples; i++) { // Loop through samples
                f32[i] = Math.max(-1, Math.min(1, i16[i] / 32768)); // Normalize to [-1, 1] range and clamp
            } // End normalization loop

            // Reuse existing AudioContext and playhead scheduling pattern
            const ctx = ensureChatterboxLipSyncGraph(); // Prepare shared audio context and analyser chain
            if (ctx.state === 'suspended') { // Resume context if autoplay policies suspended it
                try { ctx.resume(); } catch (_) {} // Ignore resume errors (will resume on user gesture)
            }

            const safeChannels = Math.max(1, channels | 0); // Ensure channel count is a positive integer
            const framesPerChannel = Math.floor(totalSamples / safeChannels); // Sample frames available for each channel
            if (framesPerChannel <= 0) {
                console.warn('⚠️ PCM16 chunk shorter than declared channel count, skipping playback');
                return;
            }
            const trailingSamples = totalSamples - framesPerChannel * safeChannels;
            if (trailingSamples !== 0) {
                console.warn('⚠️ Dropping', trailingSamples, 'sample(s) that do not fit evenly across', safeChannels, 'channels');
            }
            const buf = ctx.createBuffer(safeChannels, framesPerChannel, sampleRate); // Create audio buffer with specified channels, length, and sample rate

            // Set channel data from decoded PCM
            if (safeChannels === 1) { // If mono channel
                const channelData = buf.getChannelData(0);
                channelData.set(f32.subarray(0, framesPerChannel)); // Set single channel data
            } else { // If multi-channel (stereo or more)
                // Split interleaved samples into channel arrays
                for (let ch = 0; ch < safeChannels; ch++) { // Loop through each channel
                    const channelData = buf.getChannelData(ch); // Reference channel buffer directly
                    for (let i = 0; i < framesPerChannel; i++) { // Loop through samples in channel
                        channelData[i] = f32[i * safeChannels + ch]; // Extract interleaved sample for this channel
                    } // End sample extraction loop
                } // End channel loop
            } // End channel data assignment

            // Create and schedule audio source (same pattern as existing schedule function)
            const src = ctx.createBufferSource(); // Create buffer source node
            src.buffer = buf; // Set decoded buffer
            if (ttsAnalyserNode) { // Route through analyser when available for lip sync
                src.connect(ttsAnalyserNode);
            } else {
                src.connect(ctx.destination); // Fallback directly to destination if analyser missing
            }

            ttsPcmActiveSources += 1; // Track active sources so we know when playback has finished
            const handleEnded = () => { // Cleanup when buffer playback completes
                if (src) {
                    try { src.removeEventListener('ended', handleEnded); } catch (_) {}
                }
                if (ttsPcmActiveSources > 0) {
                    ttsPcmActiveSources -= 1;
                }
                if (ttsPcmActiveSources === 0) { // Schedule mouth close once final buffer finishes
                    stopChatterboxLipSync(false);
                }
            };
            src.addEventListener('ended', handleEnded, { once: true }); // Ensure cleanup runs only once

            // Ensure we can immediately stop scheduled sources on interruption
            ttsCleanupFns.push(() => {
                try { src.removeEventListener('ended', handleEnded); } catch(_) {}
                try { src.stop(0); } catch(_) {}
                try { src.disconnect(); } catch(_) {}
            });

            // Keep the same playhead logic (reuse from existing scheduler)
            window.__opus.playhead = Math.max(ctx.currentTime, window.__opus.playhead || 0); // Calculate start time with playhead overlap
            src.start(window.__opus.playhead); // Schedule playback at playhead position
            window.__opus.playhead += buf.duration - 0.02; // Update playhead with 20ms overlap to hide seams
            
            console.log('🔊 Scheduled PCM16 chunk:', framesPerChannel, 'frames per channel (', totalSamples, 'total samples ),', sampleRate, 'Hz,', safeChannels, 'channels, duration:', buf.duration.toFixed(3), 's'); // Log playback info
        } // End playPcm16Delta

        // OpenAI-compatible TTS function
        async function speakWithOpenAITTS(text) {
            let localGen = 0;
            let localController = null;
            try {
                if (!text) return;
                
                text = sanitizeTTS(text);
                
                const ttsEndpointInput = document.getElementById('settings-tts-endpoint');
                if (!ttsEndpointInput || !ttsEndpointInput.value || !ttsEndpointInput.value.trim()) {
                    console.error('TTS endpoint not configured');
                    textToSpeech(text); // Fallback to browser TTS
                    return;
                }
                
                const ttsEndpointBase = ttsEndpointInput.value.trim().replace(/\/$/, '');
                let baseUrl;
                try {
                    const endpointUrl = new URL(ttsEndpointBase);
                    baseUrl = endpointUrl.origin;
                } catch (e) {
                    const match = ttsEndpointBase.match(/^(https?:\/\/[^\/]+)/);
                    baseUrl = match ? match[1] : ttsEndpointBase.replace(/\/v1$/, '');
                }
                
                const endpoint = `${PROXY_BASE_URL}/v1/proxy/tts/speech?endpoint=${encodeURIComponent(baseUrl)}`;
                const ttsModelDropdown = document.getElementById('settings-tts-model');
                const ttsVoiceDropdown = document.getElementById('settings-tts-voice');
                const modelId = (ttsModelDropdown && ttsModelDropdown.value) ? ttsModelDropdown.value : 'tts-1';
                const voiceId = (ttsVoiceDropdown && ttsVoiceDropdown.value) ? ttsVoiceDropdown.value : 'alloy';

                const reqBody = {
                    model: modelId,
                    voice: voiceId,
                    input: text,
                    stream: true
                };

                const headers = {
                    'Content-Type': 'application/json',
                    'Accept': 'text/event-stream'
                };

                try { if (ttsAbortController) { ttsAbortController.abort(); } } catch (_) {}
                localController = new AbortController();
                ttsAbortController = localController;
                localGen = (++ttsGeneration);

                const res = await fetch(endpoint, {
                    method: 'POST',
                    headers: headers,
                    body: JSON.stringify(reqBody),
                    signal: localController.signal
                });

                if (localGen !== ttsGeneration || (localController && localController.signal.aborted)) {
                    console.log('TTS request cancelled');
                    stopChatterboxLipSync(true);
                    return;
                }
                
                if (!res.ok) {
                    const errText = await res.text().catch(() => '');
                    console.error('TTS error:', res.status, errText);
                    stopChatterboxLipSync(true);
                    textToSpeech(text);
                    return;
                }
                
                const ct = (res.headers.get('content-type') || '').toLowerCase();
                const isBinaryAudio = ct.includes('audio/');
                const isSSE = ct.includes('text/event-stream'); // Check if response is SSE format (Chatterbox/OpenAI streaming)
                
                // Check if SSE response actually contains binary audio (content-type mismatch)
                // Clone the response to peek at first bytes without consuming the stream
                let actualIsBinaryAudio = isBinaryAudio;
                if (isSSE && !isBinaryAudio) {
                    // Peek at the first chunk to see if it's actually binary audio
                    const responseClone = res.clone();
                    const reader = responseClone.body.getReader();
                    const firstChunk = await reader.read();
                    reader.releaseLock();
                    
                    if (!firstChunk.done && firstChunk.value) {
                        const firstBytes = new Uint8Array(firstChunk.value);
                        // Check for common audio file signatures
                        const isBinary = firstBytes.length >= 4 && (
                            (firstBytes[0] === 0x52 && firstBytes[1] === 0x49 && firstBytes[2] === 0x46 && firstBytes[3] === 0x46) || // RIFF (WAV)
                            (firstBytes[0] === 0xFF && firstBytes[1] === 0xFB) || // MP3
                            (firstBytes[0] === 0x4F && firstBytes[1] === 0x67 && firstBytes[2] === 0x67 && firstBytes[3] === 0x53) || // OggS
                            (firstBytes[0] === 0x1A && firstBytes[1] === 0x45 && firstBytes[2] === 0xDF && firstBytes[3] === 0xA3) // WebM/EBML
                        );
                        if (isBinary) {
                            console.log('🔄 Detected binary audio in SSE response (content-type mismatch), treating as binary audio');
                            actualIsBinaryAudio = true;
                        }
                    }
                }
                
                // Handle binary audio first (even if content-type says SSE but data is binary)
                if (actualIsBinaryAudio) {
                    console.log('Received binary audio format:', ct);
                    stopChatterboxLipSync(true);
                    
                    try {
                        // Ensure audio context is resumed (critical for iOS Safari)
                        await resumeAudioContextOnce();
                        
                        let mimeType = 'audio/mpeg';
                        if (ct.includes('audio/wav') || ct.includes('audio/wave')) {
                            mimeType = 'audio/wav';
                        } else if (ct.includes('audio/webm')) {
                            mimeType = 'audio/webm';
                        } else if (ct.includes('audio/ogg')) {
                            mimeType = 'audio/ogg';
                        } else if (ct.includes('audio/mp4') || ct.includes('audio/m4a')) {
                            mimeType = 'audio/mp4';
                        } else {
                            // Default to WAV if we detected RIFF header
                            mimeType = 'audio/wav';
                        }
                        
                        const audioBlob = await res.blob();
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        audio.preload = 'auto';
                        audio.crossOrigin = 'anonymous';
                        
                        // iOS Safari fix: Add audio element to DOM (required for some iOS versions)
                        audio.style.display = 'none';
                        audio.style.position = 'absolute';
                        audio.style.width = '1px';
                        audio.style.height = '1px';
                        audio.style.opacity = '0';
                        document.body.appendChild(audio);
                        
                        // Cleanup function to remove audio element from DOM
                        const cleanupAudioElement = () => {
                            try {
                                if (audio && audio.parentNode) {
                                    audio.parentNode.removeChild(audio);
                                }
                                URL.revokeObjectURL(audioUrl);
                            } catch (e) {
                                console.warn('Error cleaning up audio element:', e);
                            }
                        };
                        
                        // Update onended handler to stop lip sync when audio ends
                        audio.onended = () => {
                            ttsStreamActive = false; // Stop lip sync loop
                            cleanupAudioElement();
                            try { if (ttsLipSyncIntervalId) { clearInterval(ttsLipSyncIntervalId); ttsLipSyncIntervalId = null; } } catch(_){}
                            try { if (ttsRafId) { cancelAnimationFrame(ttsRafId); ttsRafId = 0; } } catch(_){}
                            try { ttsCleanupFns.forEach(fn => { try { fn(); } catch(_){} }); } catch(_){}
                            ttsCleanupFns = [];
                            if (live2dModel) { live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0); }
                            if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') { animateVRMLipSync(0); }
                        };
                        
                        // iOS Safari fix: Use a single global audio context and check if already connected
                        let audioContext = window.__ttsAudioContext;
                        if (!audioContext) {
                            audioContext = new (window.AudioContext || window.webkitAudioContext)();
                            window.__ttsAudioContext = audioContext;
                        }
                        
                        // iOS Safari fix: Resume audio context if suspended (required for autoplay)
                        if (audioContext.state === 'suspended') {
                            try {
                                await audioContext.resume();
                                console.log('✅ Audio context resumed for playback');
                            } catch (resumeError) {
                                console.warn('⚠️ Could not resume audio context:', resumeError);
                            }
                        }
                        
                        // iOS Safari fix: Check if audio element already has a source node
                        // createMediaElementSource can only be called once per element
                        let source = audio.__mediaElementSource;
                        if (!source) {
                            try {
                                source = audioContext.createMediaElementSource(audio);
                                audio.__mediaElementSource = source; // Cache the source
                            } catch (sourceError) {
                                // If source already exists or can't be created, try to reuse existing
                                console.warn('Could not create media element source, may already exist:', sourceError);
                                // Fallback: try to get existing source or create new audio element
                                if (sourceError.message && sourceError.message.includes('already been connected')) {
                                    // Create a new audio element if the old one was already connected
                                    const newAudio = new Audio(audioUrl);
                                    newAudio.preload = 'auto';
                                    newAudio.crossOrigin = 'anonymous';
                                    newAudio.style.display = 'none';
                                    newAudio.style.position = 'absolute';
                                    newAudio.style.width = '1px';
                                    newAudio.style.height = '1px';
                                    newAudio.style.opacity = '0';
                                    document.body.appendChild(newAudio);
                                    
                                    // Copy event handlers
                                    newAudio.onended = audio.onended;
                                    
                                    // Cleanup old audio
                                    try {
                                        if (audio && audio.parentNode) {
                                            audio.parentNode.removeChild(audio);
                                        }
                                    } catch (_) {}
                                    
                                    audio = newAudio;
                                    source = audioContext.createMediaElementSource(audio);
                                    audio.__mediaElementSource = source;
                                } else {
                                    throw sourceError;
                                }
                            }
                        }
                        
                        const analyser = audioContext.createAnalyser();
                        analyser.fftSize = 256;
                        source.connect(analyser);
                        analyser.connect(audioContext.destination);
                        
                        // Use the analyser for lip sync
                        ttsAnalyserNode = analyser;
                        
                        // Set flag to keep lip sync loop running while audio plays
                        ttsStreamActive = true;
                        
                        // Start lip sync before playing audio
                        startLipSyncFromAnalyserNode();
                        
                        // iOS Safari fix: Play audio with proper error handling
                        try {
                            // Set volume to ensure audio plays (iOS Safari sometimes requires this)
                            audio.volume = 1.0;
                            
                            // Play audio and wait for it to start
                            const playPromise = audio.play();
                            
                            if (playPromise !== undefined) {
                                await playPromise;
                                console.log('✅ Audio playback started successfully');
                            }
                            
                            console.log('Playing binary audio from TTS service');
                        } catch (playError) {
                            console.error('❌ Error playing audio:', playError);
                            ttsStreamActive = false; // Stop lip sync on error
                            
                            // iOS Safari specific: If play fails, it might be due to autoplay policy
                            // Try to provide user feedback or fallback
                            if (playError.name === 'NotAllowedError' || playError.name === 'NotSupportedError') {
                                console.warn('⚠️ Audio playback blocked by browser policy. User interaction may be required.');
                                // Cleanup and fallback to browser TTS
                                cleanupAudioElement();
                                stopChatterboxLipSync(true);
                                textToSpeech(text);
                                return;
                            }
                            
                            throw playError; // Re-throw if it's a different error
                        }
                        
                        return;
                    } catch (audioError) {
                        console.error('Error playing binary audio:', audioError);
                        stopChatterboxLipSync(true);
                        textToSpeech(text);
                        return;
                    }
                } else if (isSSE) {
                    // Handle SSE format (Chatterbox/PCM streaming) - only if it's actually SSE
                    console.log('📡 Received SSE format (Chatterbox/PCM) - processing stream'); // Log format detection
                    
                    // iOS Safari fix: Ensure audio context is resumed before starting SSE (critical for iOS)
                    await resumeAudioContextOnce();
                    
                    // Prepare analyser-driven lip sync graph for PCM streaming
                    stopChatterboxLipSync(true); // Reset any previous PCM lip sync loop before starting a new stream
                    const pcmCtx = ensureChatterboxLipSyncGraph(); // Ensure audio context and analyser exist
                    console.log('🎵 Audio context state:', pcmCtx.state, 'sampleRate:', pcmCtx.sampleRate); // Debug log
                    if (pcmCtx.state === 'suspended') { // Resume context if required by autoplay policies
                        try { 
                            await pcmCtx.resume(); 
                            console.log('✅ Audio context resumed for SSE streaming, new state:', pcmCtx.state);
                        } catch (e) {
                            console.warn('⚠️ Could not resume audio context, will resume on user gesture:', e);
                        } // Swallow resume errors; user interaction will resume if needed
                    } else {
                        console.log('✅ Audio context already running'); // Log if already active
                    }
                    ttsStreamActive = true; // Mark stream active so analyser loop stays alive between chunks
                    startLipSyncFromAnalyserNode(); // Start analyser-driven lip sync updates
                    ttsCleanupFns.push(() => { // Ensure cleanup when speech is cancelled elsewhere
                        try { stopChatterboxLipSync(true); } catch(_) {}
                        try { if (localController) { localController.abort(); } } catch(_) {}
                    });

                    if (window.__opus.playhead === undefined) { // Check if playhead exists
                        window.__opus.playhead = 0; // Initialize playhead to 0
                    } // End playhead initialization
                    
                    // Capture sample_rate and channels from speech.audio.info event
                    let ttsSampleRate = 24000; // Default sample rate (fallback)
                    let ttsChannels = 1; // Default channels (mono, fallback)
                    
                    // Wire SSE events to PCM16 decoder
                    try {
                        await streamSSE(res, { // Start SSE parsing with callbacks
                            onInfo: (evt) => { // Handle metadata/info event
                                if (localGen !== ttsGeneration || (localController && localController.signal.aborted)) { return; }
                                ttsSampleRate = evt.sample_rate || evt.sampleRate || 24000; // Capture sample rate from metadata
                                ttsChannels = evt.channels || 1; // Capture channel count from metadata
                                console.log('🎵 PCM16 audio info - sample_rate:', ttsSampleRate, 'channels:', ttsChannels); // Log audio info
                            }, // End onInfo handler
                            onInit: (b64) => { // Handle init event (if any - PCM16 may not need init)
                                if (localGen !== ttsGeneration || (localController && localController.signal.aborted)) { return; }
                                console.log('🎵 Received init event (PCM16 may not require init)'); // Log init event
                                // PCM16 typically doesn't need init segment, but handle if provided
                            }, // End onInit handler
                            onDelta: (b64) => { // Decode and play each PCM16 delta chunk
                                if (localGen !== ttsGeneration || (localController && localController.signal.aborted)) { 
                                    console.log('⚠️ Delta chunk skipped - generation mismatch or aborted');
                                    return; 
                                }
                                if (!b64 || b64.length === 0) {
                                    console.warn('⚠️ Received empty delta chunk');
                                    return;
                                }
                                console.log('🎵 Processing delta chunk, length:', b64.length, 'sampleRate:', ttsSampleRate, 'channels:', ttsChannels); // Debug log
                                playPcm16Delta(b64, ttsSampleRate, ttsChannels);
                            },
                            onDone: () => { // Handle stream completion
                                if (localGen !== ttsGeneration || (localController && localController.signal.aborted)) { return; }
                                console.log('🎵 SSE stream complete (PCM16)'); // Log completion
                                ttsStreamActive = false; // Mark stream inactive so lip sync can wind down
                                stopChatterboxLipSync(false); // Allow mouth to close after buffers finish
                                // Reset playhead for next stream (optional)
                                // window.__opus.playhead = 0; // Uncomment if you want to reset between streams
                            }, // End onDone handler
                            onError: (e) => { // Handle parsing errors
                                // Skip fallback if this was an intentional abort (cancellation) or if generation token is stale
                                if (localGen !== ttsGeneration || (e && e.name === 'AbortError') || (localController && localController.signal.aborted)) {
                                    console.log('🛑 SSE stream cancelled (intentional abort), skipping fallback'); // Log cancellation
                                    ttsStreamActive = false; // Mark stream inactive on abort
                                    stopChatterboxLipSync(true); // Immediately stop lip sync
                                    return; // Exit without fallback
                                }
                                console.error('❌ SSE parsing error:', e); // Log error
                                ttsStreamActive = false; // Mark stream inactive on error
                                stopChatterboxLipSync(false); // Schedule cleanup for analyser-driven lip sync
                                textToSpeech(text); // Fall back to browser TTS if error occurs
                            } // End onError handler
                        }); // End streamSSE call
                        return; // Exit after handling SSE stream
                    } catch (sseError) {
                        // Check if this is the binary audio in SSE stream error
                        if (sseError && sseError.message === 'BINARY_AUDIO_IN_SSE_STREAM' && sseError.audioBlob) {
                        console.log('🔄 Binary audio detected in SSE stream, using collected blob');
                        // The stream was actually binary audio, use the blob collected in streamSSE
                        stopChatterboxLipSync(true);
                        
                        try {
                            // Ensure audio context is resumed (critical for iOS Safari)
                            await resumeAudioContextOnce();
                            
                            // Use the blob that was collected in streamSSE
                            const audioBlob = sseError.audioBlob;
                            const audioUrl = URL.createObjectURL(audioBlob);
                            const audio = new Audio(audioUrl);
                            audio.preload = 'auto';
                            audio.crossOrigin = 'anonymous';
                            
                            // iOS Safari fix: Add audio element to DOM
                            audio.style.display = 'none';
                            audio.style.position = 'absolute';
                            audio.style.width = '1px';
                            audio.style.height = '1px';
                            audio.style.opacity = '0';
                            document.body.appendChild(audio);
                            
                            const cleanupAudioElement = () => {
                                try {
                                    if (audio && audio.parentNode) {
                                        audio.parentNode.removeChild(audio);
                                    }
                                    URL.revokeObjectURL(audioUrl);
                                } catch (e) {
                                    console.warn('Error cleaning up audio element:', e);
                                }
                            };
                            
                            audio.onended = () => {
                                cleanupAudioElement();
                                try { if (ttsLipSyncIntervalId) { clearInterval(ttsLipSyncIntervalId); ttsLipSyncIntervalId = null; } } catch(_){}
                                try { if (ttsRafId) { cancelAnimationFrame(ttsRafId); ttsRafId = 0; } } catch(_){}
                                try { ttsCleanupFns.forEach(fn => { try { fn(); } catch(_){} }); } catch(_){}
                                ttsCleanupFns = [];
                                if (live2dModel) { live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0); }
                                if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') { animateVRMLipSync(0); }
                            };
                            
                            // iOS Safari fix: Use global audio context
                            let audioContext = window.__ttsAudioContext;
                            if (!audioContext) {
                                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                                window.__ttsAudioContext = audioContext;
                            }
                            
                            if (audioContext.state === 'suspended') {
                                try {
                                    await audioContext.resume();
                                    console.log('✅ Audio context resumed for playback');
                                } catch (resumeError) {
                                    console.warn('⚠️ Could not resume audio context:', resumeError);
                                }
                            }
                            
                            let source = audio.__mediaElementSource;
                            if (!source) {
                                try {
                                    source = audioContext.createMediaElementSource(audio);
                                    audio.__mediaElementSource = source;
                                } catch (sourceError) {
                                    if (sourceError.message && sourceError.message.includes('already been connected')) {
                                        const newAudio = new Audio(audioUrl);
                                        newAudio.preload = 'auto';
                                        newAudio.crossOrigin = 'anonymous';
                                        newAudio.style.display = 'none';
                                        newAudio.style.position = 'absolute';
                                        newAudio.style.width = '1px';
                                        newAudio.style.height = '1px';
                                        newAudio.style.opacity = '0';
                                        document.body.appendChild(newAudio);
                                        newAudio.onended = audio.onended;
                                        try {
                                            if (audio && audio.parentNode) {
                                                audio.parentNode.removeChild(audio);
                                            }
                                        } catch (_) {}
                                        audio = newAudio;
                                        source = audioContext.createMediaElementSource(audio);
                                        audio.__mediaElementSource = source;
                                    } else {
                                        throw sourceError;
                                    }
                                }
                            }
                            
                            const analyser = audioContext.createAnalyser();
                            analyser.fftSize = 256;
                            source.connect(analyser);
                            analyser.connect(audioContext.destination);
                            
                            ttsAnalyserNode = analyser;
                            
                            // Update onended handler to stop lip sync when audio ends
                            const originalOnEnded = audio.onended;
                            audio.onended = () => {
                                ttsStreamActive = false; // Stop lip sync loop
                                if (originalOnEnded) originalOnEnded();
                                cleanupAudioElement();
                                try { if (ttsLipSyncIntervalId) { clearInterval(ttsLipSyncIntervalId); ttsLipSyncIntervalId = null; } } catch(_){}
                                try { if (ttsRafId) { cancelAnimationFrame(ttsRafId); ttsRafId = 0; } } catch(_){}
                                try { ttsCleanupFns.forEach(fn => { try { fn(); } catch(_){} }); } catch(_){}
                                ttsCleanupFns = [];
                                if (live2dModel) { live2dModel.internalModel.coreModel.setParameterValueById('ParamMouthOpenY', 0); }
                                if (vrmModel && getSetting('avatarMode', 'live2d') === 'vrm') { animateVRMLipSync(0); }
                            };
                            
                            // Set flag to keep lip sync loop running while audio plays
                            ttsStreamActive = true;
                            
                            // Start lip sync before playing audio
                            startLipSyncFromAnalyserNode();
                            
                            try {
                                audio.volume = 1.0;
                                const playPromise = audio.play();
                                if (playPromise !== undefined) {
                                    await playPromise;
                                    console.log('✅ Audio playback started successfully');
                                }
                                console.log('Playing binary audio from TTS service (detected in SSE stream)');
                            } catch (playError) {
                                console.error('❌ Error playing audio:', playError);
                                if (playError.name === 'NotAllowedError' || playError.name === 'NotSupportedError') {
                                    console.warn('⚠️ Audio playback blocked by browser policy');
                                    cleanupAudioElement();
                                    stopChatterboxLipSync(true);
                                    textToSpeech(text);
                                    return;
                                }
                                throw playError;
                            }
                            
                            return;
                        } catch (audioError) {
                            console.error('Error playing binary audio from SSE stream:', audioError);
                            stopChatterboxLipSync(true);
                            textToSpeech(text);
                            return;
                        }
                    } else {
                        throw sseError; // Re-throw other errors
                    }
                    } // End catch block for SSE errors
                } else {
                    // Unknown format - neither SSE nor binary audio
                    console.warn('⚠️ Unknown TTS response format:', ct, '- falling back to browser TTS'); // Log format mismatch
                    stopChatterboxLipSync(true); // Reset lip sync state before fallback
                    textToSpeech(text); // Fall back to browser TTS
                    return; // Exit early
                } // End format handling
            } catch (e) {
                if (localGen !== ttsGeneration) {
                    console.log('TTS request superseded');
                    ttsStreamActive = false;
                    stopChatterboxLipSync(true);
                    return;
                }
                const isAbortError = (e && (e.name === 'AbortError' || e.message?.includes('aborted'))) || 
                                    (localController && localController.signal && localController.signal.aborted && localGen === ttsGeneration);
                if (isAbortError) {
                    console.log('TTS cancelled');
                    ttsStreamActive = false;
                    stopChatterboxLipSync(true);
                    return;
                }
                console.error('TTS failed:', e);
                ttsStreamActive = false;
                stopChatterboxLipSync(false);
                textToSpeech(text);
            }
        }

        // Load Microsoft browser voices
        function loadMicrosoftVoices() {
            const microsoftVoiceDropdown = document.getElementById('settings-microsoft-voice');
            if (!microsoftVoiceDropdown) return;
            
            voices = speechSynthesis.getVoices();
            
            if (voices.length === 0) {
                console.warn('No voices available yet, waiting for voices to load');
                // Try again after voices are loaded
                if (speechSynthesis.onvoiceschanged !== undefined) {
                    speechSynthesis.onvoiceschanged = loadMicrosoftVoices;
                }
                return;
            }
            
            // Read previously selected voice from storage
            let storedVoiceURI = null;
            try {
                storedVoiceURI = storage.getItem(SELECTED_VOICE_STORAGE_KEY);
            } catch (readError) {
                console.warn('Could not read selected voice from localStorage:', readError);
            }
            
            microsoftVoiceDropdown.innerHTML = '';
            
            let defaultVoiceIndex = 0;
            
            voices.forEach((voice, index) => {
                const option = document.createElement('option');
                option.value = index;
                option.textContent = `${voice.name} (${voice.lang})`;
                
                // Check if this is the previously selected voice
                if (storedVoiceURI && voice.voiceURI === storedVoiceURI) {
                    defaultVoiceIndex = index;
                }
                
                microsoftVoiceDropdown.appendChild(option);
            });
            
            // Set the previously selected voice or default
            microsoftVoiceDropdown.selectedIndex = defaultVoiceIndex;
        }

        // Load Microsoft browser voices
        function loadMicrosoftVoices() {
            const microsoftVoiceDropdown = document.getElementById('settings-microsoft-voice');
            if (!microsoftVoiceDropdown) return;
            
            voices = speechSynthesis.getVoices();
            
            if (voices.length === 0) {
                console.warn('No voices available yet, waiting for voices to load');
                // Try again after voices are loaded
                if (speechSynthesis.onvoiceschanged !== undefined) {
                    speechSynthesis.onvoiceschanged = loadMicrosoftVoices;
                }
                return;
            }
            
            // Read previously selected voice from storage
            let storedVoiceURI = null;
            try {
                storedVoiceURI = storage.getItem(SELECTED_VOICE_STORAGE_KEY);
            } catch (readError) {
                console.warn('Could not read selected voice from localStorage:', readError);
            }
            
            microsoftVoiceDropdown.innerHTML = '';
            
            let defaultVoiceIndex = 0;
            
            voices.forEach((voice, index) => {
                const option = document.createElement('option');
                option.value = index;
                option.textContent = `${voice.name} (${voice.lang})`;
                microsoftVoiceDropdown.appendChild(option);
                
                // Prefer Microsoft Ana Online; otherwise choose first English voice as default
                if (voice.name === 'Microsoft Ana Online (Natural) - English (United States)') {
                    defaultVoiceIndex = index;
                } else if (voice.lang.includes('en-') && defaultVoiceIndex === 0) {
                    defaultVoiceIndex = index;
                }
            });
            
            // Determine which voice should be selected
            let selectedIndex = defaultVoiceIndex;
            if (storedVoiceURI) {
                const idx = voices.findIndex(v => v.voiceURI === storedVoiceURI);
                if (idx !== -1) {
                    selectedIndex = idx;
                }
            }
            
            microsoftVoiceDropdown.value = String(selectedIndex);
            
            // Persist the selection
            try {
                if (!storedVoiceURI && voices[selectedIndex]) {
                    storage.setItem(SELECTED_VOICE_STORAGE_KEY, voices[selectedIndex].voiceURI);
                }
            } catch (saveError) {
                console.warn('Could not save voice selection:', saveError);
            }
            
            // Add change listener to save selection
            microsoftVoiceDropdown.addEventListener('change', function() {
                const selectedVoiceIndex = parseInt(microsoftVoiceDropdown.value);
                if (!isNaN(selectedVoiceIndex) && voices[selectedVoiceIndex]) {
                    try {
                        storage.setItem(SELECTED_VOICE_STORAGE_KEY, voices[selectedVoiceIndex].voiceURI);
                        saveSettings();
                    } catch (e) {
                        console.warn('Could not save voice selection:', e);
                    }
                }
            });
        }

        // Fetch TTS voices from OpenAI-compatible endpoint
        async function fetchTtsVoices() {
            try {
                const ttsEndpointInput = document.getElementById('settings-tts-endpoint');
                const endpoint = (ttsEndpointInput && ttsEndpointInput.value && ttsEndpointInput.value.trim()) 
                    ? ttsEndpointInput.value.trim().replace(/\/$/, '') 
                    : 'http://localhost:4123/v1';
                
                let baseUrl;
                try {
                    const endpointUrl = new URL(endpoint);
                    baseUrl = endpointUrl.origin;
                } catch (e) {
                    const match = endpoint.match(/^(https?:\/\/[^\/]+)/);
                    baseUrl = match ? match[1] : endpoint.replace(/\/v1$/, '');
                }
                
                console.log('Fetching voices through proxy from TTS endpoint:', baseUrl);
                
                const proxyUrl = `${PROXY_BASE_URL}/v1/proxy/tts/voices?endpoint=${encodeURIComponent(baseUrl)}`;
                const response = await fetch(proxyUrl, {
                    method: 'GET',
                    headers: { 'Content-Type': 'application/json' }
                });
                
                if (!response.ok) {
                    console.error('Failed to fetch voices');
                    return;
                }
                
                const responseData = await response.json();
                let voicesData = responseData;
                if (responseData && typeof responseData === 'object' && responseData.voices) {
                    voicesData = responseData.voices;
                } else if (!Array.isArray(responseData)) {
                    console.warn('Unexpected voices response format:', responseData);
                    voicesData = [];
                }
                
                const ttsVoiceDropdown = document.getElementById('settings-tts-voice');
                if (!ttsVoiceDropdown) return;
                
                const storedVoice = getSetting('ttsVoice', '');
                ttsVoiceDropdown.innerHTML = '';
                
                if (Array.isArray(voicesData) && voicesData.length > 0) {
                    voicesData.forEach(voice => {
                        const option = document.createElement('option');
                        const voiceValue = typeof voice === 'string' ? voice : (voice.id || voice.name || voice);
                        const voiceLabel = typeof voice === 'string' ? voice : (voice.name || voice.id || voice);
                        option.value = voiceValue;
                        option.textContent = voiceLabel;
                        ttsVoiceDropdown.appendChild(option);
                    });
                } else {
                    const defaultVoices = ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer', 'Empress'];
                    defaultVoices.forEach(voice => {
                        const option = document.createElement('option');
                        option.value = voice;
                        option.textContent = voice;
                        ttsVoiceDropdown.appendChild(option);
                    });
                }
                
                const availableVoiceValues = Array.from(ttsVoiceDropdown.options).map(option => option.value);
                if (storedVoice && availableVoiceValues.includes(storedVoice)) {
                    ttsVoiceDropdown.value = storedVoice;
                } else if (ttsVoiceDropdown.options.length > 0) {
                    ttsVoiceDropdown.value = ttsVoiceDropdown.options[0].value;
                }
                
                console.log('TTS voices updated successfully');
            } catch (error) {
                console.error('Error fetching TTS voices:', error);
            }
        }

        // Send button handler
        sendBtn.addEventListener('click', async function () {
            const promptText = userInput.value.trim();
            if (promptText && detectPhilosopherModeTrigger(promptText)) {
                await startPhilosopherMode();
                userInput.value = '';
                return;
            }

            const shouldStopPhilosopher = (philosopherModeActive || philosopherModeStarting) && promptText;
            if (shouldStopPhilosopher) {
                await stopPhilosopherMode(true);
            }
            await resumeAudioContextOnce();
            const userText = userInput.value;
            if (userText.trim() === '') {
                showStatus('Please enter some text or record your voice.', 2000);
                return;
            }
            
            fetchOpenAIResponse(userText);
            
            if (shouldStopPhilosopher) {
                addMessageToHistory('assistant', 'Philosopher Mode deactivated.');
            }
            userInput.value = '';
            userInput.style.height = 'auto';
        });

        // Enter key handler
        userInput.addEventListener('keydown', async function (event) {
            if (event.key === 'Enter' && !event.shiftKey) {
                event.preventDefault();
                await resumeAudioContextOnce();
                const userText = userInput.value;
                
                if (userText.trim() && detectPhilosopherModeTrigger(userText)) {
                    await startPhilosopherMode();
                    userInput.value = '';
                    userInput.style.height = 'auto';
                    return;
                }

                const shouldStopPhilosopher = (philosopherModeActive || philosopherModeStarting) && userText.trim();
                if (shouldStopPhilosopher) {
                    await stopPhilosopherMode(true);
                }
                
                if (userText.trim() !== '') {
                    fetchOpenAIResponse(userText);
                    userInput.value = '';
                    userInput.style.height = 'auto';
                    
                    if (shouldStopPhilosopher) {
                        addMessageToHistory('assistant', 'Philosopher Mode deactivated.');
                    }
                }
            }
        });

        // ============================================================================
        // SETTINGS MANAGEMENT
        // ============================================================================
        
        function loadToolSettings() {
            try {
                const savedSettings = storage.getItem('toolSettings');
                if (savedSettings) {
                    const settings = JSON.parse(savedSettings);
                    // Settings will be used by getApiKey, getEndpoint, etc.
                    console.log('Tool settings loaded from localStorage');
                }
            } catch (error) {
                console.warn('Error loading tool settings:', error);
            }
        }

        function escapeHtml(text) {
            if (text === null || text === undefined) return '';
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function renderSettings() {
            const settings = {
                userName: getSetting('userName', 'User'),
                assistantName: getSetting('assistantName', 'EVA'),
                apiKey: getSetting('apiKey', ''),
                endpoint: getSetting('endpoint', 'http://localhost:1234/v1/chat/completions'),
                newsApiKey: getSetting('newsApiKey', ''),
                systemPrompt: getSetting('systemPrompt', 'You are a friendly AI assistant called EVA.'),
                webcamMode: getSetting('webcamMode', false),
                clipboardMode: getSetting('clipboardMode', false),
                muteMode: getSetting('muteMode', false),
                baseModel: getSetting('baseModel', ''),
                toolModel: getSetting('toolModel', ''),
                visionModel: getSetting('visionModel', ''),
                ttsService: getSetting('ttsService', 'microsoft'),
                ttsEndpoint: getSetting('ttsEndpoint', 'http://localhost:4123/v1'),
                ttsModel: getSetting('ttsModel', 'tts-1'),
                ttsVoice: getSetting('ttsVoice', 'Empress'),
                vrmVersion: getSetting('vrmVersion', '1.0'),
                live2dModelList: getSetting('live2dModelList', './model_avatar/KITU17/KITU17.model3.json\n./model_avatar/NVPU-demo/NVPU.model3.json\n./model_avatar/RACOON01/RACOON01.model3.json'),
                live2dOffset: getSetting('live2dOffset', 0),
                avatarMode: getSetting('avatarMode', 'live2d'),
                vrmModelList: getSetting('vrmModelList', './model_avatar/Eva/Eva.vrm\n./model_avatar/Greyson/Greyson.vrm\n./model_avatar/Bianca/Bianca.vrm\n./model_avatar/Esme/Esme.vrm'),
                vrmScale: getSetting('vrmScale', 1.0),
                vrmPositionX: getSetting('vrmPositionX', 0),
                vrmPositionY: getSetting('vrmPositionY', 0),
                vrmRotation: getSetting('vrmRotation', 0)
            };

            settingsContent.innerHTML = `
                <div class="settings-section">
                    <div class="settings-section-title">User & Assistant</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-user-name">User Name:</label>
                        <input type="text" class="settings-input" id="settings-user-name" value="${escapeHtml(settings.userName)}" placeholder="Your name">
                        <span class="helper-text">Your name shown in message history</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-assistant-name">Assistant Name:</label>
                        <input type="text" class="settings-input" id="settings-assistant-name" value="${escapeHtml(settings.assistantName)}" placeholder="Assistant name">
                        <span class="helper-text">Assistant name shown in message history</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">API Configuration</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-api-key">OpenAI API Key:</label>
                        <input type="password" class="settings-input" id="settings-api-key" value="${escapeHtml(settings.apiKey)}" placeholder="Enter your API key">
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-endpoint">API Endpoint:</label>
                        <input type="text" class="settings-input" id="settings-endpoint" value="${escapeHtml(settings.endpoint)}" placeholder="API endpoint URL">
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-news-api-key">News API Key:</label>
                        <input type="password" class="settings-input" id="settings-news-api-key" value="${escapeHtml(settings.newsApiKey)}" placeholder="Enter your News API key">
                        <span class="helper-text">Get your key from https://newsapi.org</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">System Prompt</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-system-prompt">System Prompt:</label>
                        <textarea class="settings-textarea" id="settings-system-prompt" placeholder="You are a friendly AI assistant called EVA.">${escapeHtml(settings.systemPrompt)}</textarea>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">Features</div>
                    <div class="toggle-switch">
                        <label class="settings-label" for="settings-webcam-mode">Webcam Mode:</label>
                        <label class="switch">
                            <input type="checkbox" id="settings-webcam-mode" ${settings.webcamMode ? 'checked' : ''}>
                            <span class="slider"></span>
                        </label>
                    </div>
                    <div class="toggle-switch">
                        <label class="settings-label" for="settings-clipboard-mode">Clipboard Vision Mode:</label>
                        <label class="switch">
                            <input type="checkbox" id="settings-clipboard-mode" ${settings.clipboardMode ? 'checked' : ''}>
                            <span class="slider"></span>
                        </label>
                    </div>
                    <div class="toggle-switch">
                        <label class="settings-label" for="settings-mute-mode">Mute TTS:</label>
                        <label class="switch">
                            <input type="checkbox" id="settings-mute-mode" ${settings.muteMode ? 'checked' : ''}>
                            <span class="slider"></span>
                        </label>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">Models</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-base-model">Base Chat Model:</label>
                        <select class="settings-select" id="settings-base-model">
                            <option value="">Loading models...</option>
                        </select>
                        <span class="helper-text">Model used for regular chat (default model)</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-tool-model">Tool Processing Model:</label>
                        <select class="settings-select" id="settings-tool-model">
                            <option value="">Loading models...</option>
                        </select>
                        <span class="helper-text">Model used for processing tool requests</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vision-model">Vision Model (Clipboard/Webcam):</label>
                        <select class="settings-select" id="settings-vision-model">
                            <option value="">Loading models...</option>
                        </select>
                        <span class="helper-text">Model used when webcam or clipboard image mode is enabled</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">Text to Speech</div>
                    <div class="settings-group">
                        <label class="settings-label">TTS Service:</label>
                        <div style="display: flex; gap: 16px; margin-bottom: 12px;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                <input type="radio" name="settings-tts-service" value="microsoft" ${settings.ttsService === 'microsoft' ? 'checked' : ''} id="settings-tts-service-microsoft">
                                <span>Microsoft</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                <input type="radio" name="settings-tts-service" value="openai" ${settings.ttsService === 'openai' ? 'checked' : ''} id="settings-tts-service-openai">
                                <span>OpenAI-compatible</span>
                            </label>
                        </div>
                        <span class="helper-text">Choose between Microsoft browser TTS or local Chatterbox TTS API</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-microsoft-voice">Microsoft Voice:</label>
                        <select class="settings-select" id="settings-microsoft-voice">
                            <option value="">Loading voices...</option>
                        </select>
                        <span class="helper-text">Select a Microsoft browser voice for TTS</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-tts-endpoint">TTS API Endpoint:</label>
                        <input type="text" class="settings-input" id="settings-tts-endpoint" value="${escapeHtml(settings.ttsEndpoint)}" placeholder="http://localhost:4123/v1">
                        <span class="helper-text">Endpoint for OpenAI-compatible TTS service</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-tts-model">TTS Model:</label>
                        <select class="settings-select" id="settings-tts-model">
                            <option value="tts-1" ${settings.ttsModel === 'tts-1' ? 'selected' : ''}>tts-1</option>
                            <option value="tts-1-hd" ${settings.ttsModel === 'tts-1-hd' ? 'selected' : ''}>tts-1-hd</option>
                        </select>
                        <span class="helper-text">Model for OpenAI-compatible TTS (tts-1 is faster, tts-1-hd is higher quality)</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-tts-voice">TTS Voice:</label>
                        <div style="display: flex; gap: 8px; align-items: center;">
                            <select class="settings-select" id="settings-tts-voice" style="flex: 1;">
                                <option value="">Loading voices...</option>
                            </select>
                            <button type="button" id="refresh-tts-voices-btn" class="action-btn" style="width: auto; padding: 8px 12px;" title="Refresh voices from TTS endpoint">
                                <i class="fas fa-sync-alt"></i>
                            </button>
                        </div>
                        <span class="helper-text">Voice for OpenAI-compatible TTS</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">Avatar Mode</div>
                    <div class="settings-group">
                        <label class="settings-label">Avatar Mode:</label>
                        <div style="display: flex; gap: 16px; margin-bottom: 12px;">
                            <label style="display: flex; align-items: center; gap: 8px;">
                                <input type="radio" name="settings-avatar-mode" value="live2d" ${settings.avatarMode === 'live2d' ? 'checked' : ''} id="settings-avatar-mode-live2d">
                                <span>Live2D</span>
                            </label>
                            <label style="display: flex; align-items: center; gap: 8px;">
                                <input type="radio" name="settings-avatar-mode" value="vrm" ${settings.avatarMode === 'vrm' ? 'checked' : ''} id="settings-avatar-mode-vrm">
                                <span>VRM</span>
                            </label>
                        </div>
                        <span class="helper-text">Switch between 2D Live2D and 3D VRM avatar modes</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">Live2D Settings</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-live2d-model">Live2D Model:</label>
                        <select class="settings-select" id="settings-live2d-model">
                            <option value="">Loading models...</option>
                        </select>
                        <span class="helper-text">Select which Live2D model to use</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-live2d-model-list">Live2D Model Paths (one per line):</label>
                        <textarea class="settings-textarea" id="settings-live2d-model-list" placeholder="./model_avatar/NAME/FILE.model3.json">${escapeHtml(settings.live2dModelList)}</textarea>
                        <span class="helper-text">Add new model paths on new lines to make them available</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-live2d-offset">Live2D Vertical Offset (px): <span id="settings-live2d-offset-value">${settings.live2dOffset}</span></label>
                        <input type="range" class="settings-input" id="settings-live2d-offset" min="-400" max="400" step="5" value="${settings.live2dOffset}" style="width: 100%;">
                        <span class="helper-text">Fine-tune the model's vertical position</span>
                    </div>
                </div>
                
                <div class="settings-section">
                    <div class="settings-section-title">VRM Settings</div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-version">VRM Version:</label>
                        <select class="settings-select" id="settings-vrm-version">
                            <option value="1.0" ${settings.vrmVersion === '1.0' ? 'selected' : ''}>VRM 1.0</option>
                            <option value="0.0" ${settings.vrmVersion === '0.0' ? 'selected' : ''}>VRM 0.0</option>
                        </select>
                        <span class="helper-text">Select VRM format version (1.0 is newer, 0.0 is legacy)</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-model">VRM Model:</label>
                        <select class="settings-select" id="settings-vrm-model">
                            <option value="">Loading models...</option>
                        </select>
                        <span class="helper-text">Select which VRM model to use</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-model-list">VRM Model Paths (one per line):</label>
                        <textarea class="settings-textarea" id="settings-vrm-model-list" placeholder="./model_avatar/NAME/FILE.vrm">${escapeHtml(settings.vrmModelList)}</textarea>
                        <span class="helper-text">Add new VRM model paths on new lines to make them available</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-scale">VRM Scale: <span id="settings-vrm-scale-value">${settings.vrmScale}</span></label>
                        <input type="range" class="settings-input" id="settings-vrm-scale" min="0.1" max="6.0" step="0.1" value="${settings.vrmScale}" style="width: 100%;">
                        <span class="helper-text">Adjust the 3D model's scale</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-position-x">VRM Position X: <span id="settings-vrm-position-x-value">${settings.vrmPositionX}</span></label>
                        <input type="range" class="settings-input" id="settings-vrm-position-x" min="-10" max="10" step="0.1" value="${settings.vrmPositionX}" style="width: 100%;">
                        <span class="helper-text">Adjust the 3D model's horizontal position</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-position-y">VRM Position Y: <span id="settings-vrm-position-y-value">${settings.vrmPositionY}</span></label>
                        <input type="range" class="settings-input" id="settings-vrm-position-y" min="-10" max="10" step="0.1" value="${settings.vrmPositionY}" style="width: 100%;">
                        <span class="helper-text">Adjust the 3D model's vertical position</span>
                    </div>
                    <div class="settings-group">
                        <label class="settings-label" for="settings-vrm-rotation">VRM Rotation: <span id="settings-vrm-rotation-value">${settings.vrmRotation}</span></label>
                        <input type="range" class="settings-input" id="settings-vrm-rotation" min="-180" max="180" step="1" value="${settings.vrmRotation}" style="width: 100%;">
                        <span class="helper-text">Rotate the 3D model (degrees)</span>
                    </div>
                </div>
            `;

            // Add event listeners for all settings
            function saveSettings() {
                try {
                    const currentSettings = JSON.parse(storage.getItem('toolSettings') || '{}');
                    const newSettings = {
                        ...currentSettings,
                        userName: document.getElementById('settings-user-name').value,
                        assistantName: document.getElementById('settings-assistant-name').value,
                        apiKey: document.getElementById('settings-api-key').value,
                        endpoint: document.getElementById('settings-endpoint').value,
                        newsApiKey: document.getElementById('settings-news-api-key').value,
                        systemPrompt: document.getElementById('settings-system-prompt').value,
                        webcamMode: document.getElementById('settings-webcam-mode').checked,
                        clipboardMode: document.getElementById('settings-clipboard-mode').checked,
                        muteMode: document.getElementById('settings-mute-mode').checked,
                        baseModel: document.getElementById('settings-base-model').value,
                        toolModel: document.getElementById('settings-tool-model').value,
                        visionModel: document.getElementById('settings-vision-model').value,
                        ttsService: document.querySelector('input[name="settings-tts-service"]:checked')?.value || 'microsoft',
                        ttsEndpoint: document.getElementById('settings-tts-endpoint').value,
                        ttsModel: document.getElementById('settings-tts-model').value,
                        ttsVoice: document.getElementById('settings-tts-voice').value,
                        vrmVersion: document.getElementById('settings-vrm-version').value,
                        live2dModelList: document.getElementById('settings-live2d-model-list').value,
                        live2dOffset: parseInt(document.getElementById('settings-live2d-offset').value),
                        avatarMode: document.querySelector('input[name="settings-avatar-mode"]:checked')?.value || 'live2d',
                        vrmModelList: document.getElementById('settings-vrm-model-list').value,
                        vrmScale: parseFloat(document.getElementById('settings-vrm-scale').value),
                        vrmPositionX: parseFloat(document.getElementById('settings-vrm-position-x').value),
                        vrmPositionY: parseFloat(document.getElementById('settings-vrm-position-y').value),
                        vrmRotation: parseInt(document.getElementById('settings-vrm-rotation').value)
                    };
                    storage.setItem('toolSettings', JSON.stringify(newSettings));
                    console.log('Settings saved');
                    
                    // Update isMuted if mute mode changed
                    isMuted = newSettings.muteMode;
                } catch (error) {
                    console.warn('Error saving settings:', error);
                }
            }

            // Add listeners to all inputs
            const inputs = settingsContent.querySelectorAll('input, select, textarea');
            inputs.forEach(input => {
                input.addEventListener('input', saveSettings);
                input.addEventListener('change', saveSettings);
            });

            // Fetch models when API key or endpoint changes
            const apiKeyInput = document.getElementById('settings-api-key');
            const endpointInput = document.getElementById('settings-endpoint');
            
            if (apiKeyInput) {
                apiKeyInput.addEventListener('change', () => {
                    saveSettings();
                    fetchAvailableModels();
                });
            }
            
            if (endpointInput) {
                endpointInput.addEventListener('change', () => {
                    saveSettings();
                    fetchAvailableModels();
                });
            }

            // TTS voice refresh button
            const refreshTtsVoicesBtn = document.getElementById('refresh-tts-voices-btn');
            if (refreshTtsVoicesBtn) {
                refreshTtsVoicesBtn.addEventListener('click', () => {
                    fetchTtsVoices();
                });
            }

            // Load Microsoft browser voices when settings render
            loadMicrosoftVoices();

            // Initialize TTS voices on settings render
            fetchTtsVoices();

            // Update range value displays and apply changes
            const live2dOffsetRange = document.getElementById('settings-live2d-offset');
            const live2dOffsetValue = document.getElementById('settings-live2d-offset-value');
            if (live2dOffsetRange && live2dOffsetValue) {
                // Load saved offset for current model
                const L2D_OFFSETS_KEY = 'live2dOffsets';
                try {
                    const savedOffsets = storage.getItem(L2D_OFFSETS_KEY);
                    if (savedOffsets) {
                        live2dOffsets = JSON.parse(savedOffsets);
                    }
                } catch (e) {
                    console.warn('Error loading Live2D offsets:', e);
                }
                const currentOffset = live2dOffsets[modelPath] ?? getSetting('live2dOffset', 0);
                live2dOffsetRange.value = currentOffset;
                live2dOffsetValue.textContent = currentOffset;
                
                live2dOffsetRange.addEventListener('input', (e) => {
                    const offset = parseInt(e.target.value) || 0;
                    live2dOffsetValue.textContent = offset;
                    // Update Live2D model offset if active
                    if (live2dModel && modelPath) {
                        live2dOffsets[modelPath] = offset;
                        try {
                            storage.setItem(L2D_OFFSETS_KEY, JSON.stringify(live2dOffsets));
                        } catch {}
                        // Update model position in real-time
                        const container = document.getElementById('live2d-container');
                        if (container && live2dModel) {
                            const baseY = container.clientHeight / 1.4;
                            live2dModel.y = baseY + offset;
                        }
                    }
                    saveSettings();
                });
            }

            // Load persisted VRM positions
            const VRM_POSITIONS_KEY = 'vrmPositions';
            try {
                const savedPositions = storage.getItem(VRM_POSITIONS_KEY);
                if (savedPositions) {
                    vrmPositions = JSON.parse(savedPositions);
                }
            } catch (e) {
                console.warn('Error loading VRM positions:', e);
            }

            // Get current model path (from dropdown or storage)
            let currentModelPath = currentVRMModelPath;
            if (!currentModelPath) {
                try {
                    const VRM_SELECTED_KEY_LOCAL = 'vrmSelectedModelPath';
                    currentModelPath = storage.getItem(VRM_SELECTED_KEY_LOCAL) || '';
                } catch (e) {}
            }

            // Load current model's positions
            const currentPositions = vrmPositions[currentModelPath] || { 
                scale: getSetting('vrmScale', 1.0), 
                positionX: getSetting('vrmPositionX', 0), 
                positionY: getSetting('vrmPositionY', 0), 
                rotation: getSetting('vrmRotation', 0) 
            };

            const vrmScaleRange = document.getElementById('settings-vrm-scale');
            const vrmScaleValue = document.getElementById('settings-vrm-scale-value');
            if (vrmScaleRange && vrmScaleValue) {
                // Set initial value from saved positions or settings
                vrmScaleRange.value = currentPositions.scale || getSetting('vrmScale', 1.0);
                vrmScaleValue.textContent = vrmScaleRange.value;
                
                vrmScaleRange.addEventListener('input', (e) => {
                    const newScale = parseFloat(e.target.value) || 1.0;
                    vrmScaleValue.textContent = newScale;
                    const modelPath = currentVRMModelPath || currentModelPath;
                    if (modelPath) {
                        const existing = vrmPositions[modelPath] || { scale: 1.0, positionX: 0, positionY: 0, rotation: 0 };
                        vrmPositions[modelPath] = { ...existing, scale: newScale };
                        updateVRMTransform();
                    }
                    saveSettings();
                });
            }

            const vrmPosXRange = document.getElementById('settings-vrm-position-x');
            const vrmPosXValue = document.getElementById('settings-vrm-position-x-value');
            if (vrmPosXRange && vrmPosXValue) {
                // Set initial value from saved positions or settings
                vrmPosXRange.value = currentPositions.positionX || getSetting('vrmPositionX', 0);
                vrmPosXValue.textContent = vrmPosXRange.value;
                
                vrmPosXRange.addEventListener('input', (e) => {
                    const newX = parseFloat(e.target.value) || 0;
                    vrmPosXValue.textContent = newX;
                    const modelPath = currentVRMModelPath || currentModelPath;
                    if (modelPath) {
                        const existing = vrmPositions[modelPath] || { scale: 1.0, positionX: 0, positionY: 0, rotation: 0 };
                        vrmPositions[modelPath] = { ...existing, positionX: newX };
                        updateVRMTransform();
                    }
                    saveSettings();
                });
            }

            const vrmPosYRange = document.getElementById('settings-vrm-position-y');
            const vrmPosYValue = document.getElementById('settings-vrm-position-y-value');
            if (vrmPosYRange && vrmPosYValue) {
                // Set initial value from saved positions or settings
                vrmPosYRange.value = currentPositions.positionY || getSetting('vrmPositionY', 0);
                vrmPosYValue.textContent = vrmPosYRange.value;
                
                vrmPosYRange.addEventListener('input', (e) => {
                    const newY = parseFloat(e.target.value) || 0;
                    vrmPosYValue.textContent = newY;
                    const modelPath = currentVRMModelPath || currentModelPath;
                    if (modelPath) {
                        const existing = vrmPositions[modelPath] || { scale: 1.0, positionX: 0, positionY: 0, rotation: 0 };
                        vrmPositions[modelPath] = { ...existing, positionY: newY };
                        updateVRMTransform();
                    }
                    saveSettings();
                });
            }

            const vrmRotRange = document.getElementById('settings-vrm-rotation');
            const vrmRotValue = document.getElementById('settings-vrm-rotation-value');
            if (vrmRotRange && vrmRotValue) {
                // Set initial value from saved positions or settings
                vrmRotRange.value = currentPositions.rotation || getSetting('vrmRotation', 0);
                vrmRotValue.textContent = vrmRotRange.value;
                
                vrmRotRange.addEventListener('input', (e) => {
                    const newRotation = parseInt(e.target.value, 10) || 0;
                    vrmRotValue.textContent = newRotation;
                    const modelPath = currentVRMModelPath || currentModelPath;
                    if (modelPath) {
                        const existing = vrmPositions[modelPath] || { scale: 1.0, positionX: 0, positionY: 0, rotation: 0 };
                        vrmPositions[modelPath] = { ...existing, rotation: newRotation };
                        updateVRMTransform();
                    }
                    saveSettings();
                });
            }

            // Populate Live2D model dropdown
            const live2dModelList = document.getElementById('settings-live2d-model-list');
            const live2dModelDropdown = document.getElementById('settings-live2d-model');
            const L2D_LIST_KEY = 'live2dModelList';
            const L2D_SELECTED_KEY = 'live2dSelectedModelPath';
            
            // Load saved model path
            try {
                const savedPath = storage.getItem(L2D_SELECTED_KEY);
                if (savedPath) {
                    modelPath = savedPath;
                }
            } catch (e) {
                console.warn('Error loading saved Live2D model path:', e);
            }

            function populateLive2DDropdown() {
                if (!live2dModelList || !live2dModelDropdown) return;

                const lines = live2dModelList.value
                    .split(/\r?\n/)
                    .map(l => l.trim())
                    .filter(l => l.length > 0 && l.toLowerCase().endsWith('.model3.json'));

                // Get saved selection from storage or settings
                let currentSelected = '';
                try {
                    currentSelected = storage.getItem(L2D_SELECTED_KEY) || modelPath;
                } catch (e) {
                    currentSelected = modelPath;
                }
                
                live2dModelDropdown.innerHTML = lines
                    .map(path => {
                        const fileName = path.split('/').pop();
                        const selected = path === currentSelected ? 'selected' : '';
                        return `<option value="${path}" ${selected}>${fileName}</option>`;
                    })
                    .join('');

                // If current selection is not in list but exists, add it
                if (currentSelected && !lines.includes(currentSelected) && currentSelected.toLowerCase().endsWith('.model3.json')) {
                    const fileName = currentSelected.split('/').pop();
                    const opt = document.createElement('option');
                    opt.value = currentSelected;
                    opt.textContent = fileName;
                    opt.selected = true;
                    live2dModelDropdown.appendChild(opt);
                }

                if (lines.length > 0 && !live2dModelDropdown.value) {
                    live2dModelDropdown.value = currentSelected && lines.includes(currentSelected) ? currentSelected : lines[0];
                }
            }

            if (live2dModelList && live2dModelDropdown) {
                // Initial population
                populateLive2DDropdown();

                // Update dropdown when textarea changes
                live2dModelList.addEventListener('input', () => {
                    populateLive2DDropdown();
                    saveSettings();
                });

                // Handle model selection change
                live2dModelDropdown.addEventListener('change', async function() {
                    const selectedPath = this.value;
                    if (selectedPath) {
                        try {
                            storage.setItem(L2D_SELECTED_KEY, selectedPath);
                            const currentSettings = JSON.parse(storage.getItem('toolSettings') || '{}');
                            currentSettings.live2dSelectedModel = selectedPath;
                            storage.setItem('toolSettings', JSON.stringify(currentSettings));
                            
                            // Check if Live2D mode is active by checking radio button
                            const live2dModeRadio = document.getElementById('settings-avatar-mode-live2d');
                            if (live2dModeRadio && live2dModeRadio.checked) {
                                modelPath = selectedPath;
                                try {
                                    cleanupLive2D();
                                    await initLive2D();
                                } catch (e) {
                                    console.warn('Error reinitializing Live2D model:', e);
                                }
                            }
                        } catch (e) {
                            console.warn('Error saving Live2D model selection:', e);
                        }
                    }
                });
            }

            // Populate VRM model dropdown
            const vrmModelList = document.getElementById('settings-vrm-model-list');
            const vrmModelDropdown = document.getElementById('settings-vrm-model');
            const VRM_LIST_KEY = 'vrmModelList';
            const VRM_SELECTED_KEY = 'vrmSelectedModelPath';
            
            // Load saved VRM model path
            try {
                const savedPath = storage.getItem(VRM_SELECTED_KEY);
                if (savedPath) {
                    currentVRMModelPath = savedPath;
                }
            } catch (e) {
                console.warn('Error loading saved VRM model path:', e);
            }

            function populateVRMDropdown() {
                if (!vrmModelList || !vrmModelDropdown) return;

                const lines = vrmModelList.value
                    .split(/\r?\n/)
                    .map(l => l.trim())
                    .filter(l => l.length > 0 && l.toLowerCase().endsWith('.vrm'));

                // Get saved selection from storage or settings
                let currentSelected = '';
                try {
                    currentSelected = storage.getItem(VRM_SELECTED_KEY) || currentVRMModelPath;
                } catch (e) {
                    currentSelected = currentVRMModelPath;
                }
                
                vrmModelDropdown.innerHTML = lines
                    .map(path => {
                        const fileName = path.split('/').pop();
                        const selected = path === currentSelected ? 'selected' : '';
                        return `<option value="${path}" ${selected}>${fileName}</option>`;
                    })
                    .join('');

                // If current selection is not in list but exists, add it
                if (currentSelected && !lines.includes(currentSelected) && currentSelected.toLowerCase().endsWith('.vrm')) {
                    const fileName = currentSelected.split('/').pop();
                    const opt = document.createElement('option');
                    opt.value = currentSelected;
                    opt.textContent = fileName;
                    opt.selected = true;
                    vrmModelDropdown.appendChild(opt);
                }

                if (lines.length > 0 && !vrmModelDropdown.value) {
                    vrmModelDropdown.value = currentSelected && lines.includes(currentSelected) ? currentSelected : lines[0];
                }
            }

            if (vrmModelList && vrmModelDropdown) {
                // Initial population
                populateVRMDropdown();

                // Update dropdown when textarea changes
                vrmModelList.addEventListener('input', () => {
                    populateVRMDropdown();
                    saveSettings();
                });

                // Handle model selection change
                vrmModelDropdown.addEventListener('change', async () => {
                    const selectedPath = vrmModelDropdown.value;
                    if (selectedPath) {
                        try {
                            storage.setItem(VRM_SELECTED_KEY, selectedPath);
                            const currentSettings = JSON.parse(storage.getItem('toolSettings') || '{}');
                            currentSettings.vrmSelectedModel = selectedPath;
                            storage.setItem('toolSettings', JSON.stringify(currentSettings));
                            
                            // Check if VRM mode is active by checking radio button
                            const vrmModeRadio = document.getElementById('settings-avatar-mode-vrm');
                            if (vrmModeRadio && vrmModeRadio.checked) {
                                currentVRMModelPath = selectedPath;
                                try {
                                    cleanupVRM();
                                    await initVRM();
                                } catch (e) {
                                    console.warn('Error reinitializing VRM model:', e);
                                }
                            }
                        } catch (e) {
                            console.warn('Error saving VRM model selection:', e);
                        }
                    }
                });
            }

            // Add event listeners for avatar mode radio buttons
            const live2dModeRadio = document.getElementById('settings-avatar-mode-live2d');
            const vrmModeRadio = document.getElementById('settings-avatar-mode-vrm');
            
            if (live2dModeRadio) {
                live2dModeRadio.addEventListener('change', async () => {
                    if (live2dModeRadio.checked) {
                        saveSettings(); // Save the mode change first
                        await switchToLive2D();
                    }
                });
            }
            
            if (vrmModeRadio) {
                vrmModeRadio.addEventListener('change', async () => {
                    if (vrmModeRadio.checked) {
                        saveSettings(); // Save the mode change first
                        await switchToVRM();
                    }
                });
            }
        }

        // Model loading functionality
        let availableModels = [];
        let defaultBaseModel = '';
        let defaultToolModel = '';
        let defaultVisionModel = '';

        async function fetchAvailableModels() {
            const endpoint = getEndpoint();
            const apiKey = getApiKey();
            
            if (!apiKey || !endpoint) {
                console.warn('API key or endpoint not configured, skipping model fetch');
                // Update dropdowns to show error message
                const baseModelSelect = document.getElementById('settings-base-model');
                const toolModelSelect = document.getElementById('settings-tool-model');
                const visionModelSelect = document.getElementById('settings-vision-model');
                
                if (baseModelSelect) {
                    baseModelSelect.innerHTML = '<option value="">Please configure API key and endpoint</option>';
                }
                if (toolModelSelect) {
                    toolModelSelect.innerHTML = '<option value="">Please configure API key and endpoint</option>';
                }
                if (visionModelSelect) {
                    visionModelSelect.innerHTML = '<option value="">Please configure API key and endpoint</option>';
                }
                return;
            }

            // Convert endpoint from /chat/completions to /models
            let modelsEndpoint = endpoint.replace('/chat/completions', '/models');
            
            // If endpoint doesn't contain /chat/completions, try to append /models
            if (modelsEndpoint === endpoint && !endpoint.endsWith('/models')) {
                modelsEndpoint = endpoint.endsWith('/') ? endpoint + 'models' : endpoint + '/models';
            }

            // Update dropdowns to show loading state
            const baseModelSelect = document.getElementById('settings-base-model');
            const toolModelSelect = document.getElementById('settings-tool-model');
            const visionModelSelect = document.getElementById('settings-vision-model');
            
            if (baseModelSelect) {
                baseModelSelect.innerHTML = '<option value="">Loading models...</option>';
            }
            if (toolModelSelect) {
                toolModelSelect.innerHTML = '<option value="">Loading models...</option>';
            }
            if (visionModelSelect) {
                visionModelSelect.innerHTML = '<option value="">Loading models...</option>';
            }

            try {
                console.log('Fetching models from:', modelsEndpoint);
                const response = await fetch(modelsEndpoint, {
                    method: 'GET',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    }
                });

                if (!response.ok) {
                    console.error('Failed to fetch models:', response.status, response.statusText);
                    const errorMsg = `Failed to load models: ${response.status} ${response.statusText}`;
                    if (baseModelSelect) {
                        baseModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                    }
                    if (toolModelSelect) {
                        toolModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                    }
                    if (visionModelSelect) {
                        visionModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                    }
                    return;
                }

                const data = await response.json();
                availableModels = data.data || [];
                
                if (availableModels.length === 0) {
                    console.warn('No models found in response');
                    if (baseModelSelect) {
                        baseModelSelect.innerHTML = '<option value="">No models available</option>';
                    }
                    if (toolModelSelect) {
                        toolModelSelect.innerHTML = '<option value="">No models available</option>';
                    }
                    if (visionModelSelect) {
                        visionModelSelect.innerHTML = '<option value="">No models available</option>';
                    }
                    return;
                }
                
                // Update dropdowns if they exist
                if (baseModelSelect) {
                    const currentValue = baseModelSelect.value || getSetting('baseModel', '');
                    baseModelSelect.innerHTML = availableModels
                        .map(model => `<option value="${model.id}" ${model.id === currentValue ? 'selected' : ''}>${model.id}</option>`)
                        .join('');
                    if (!baseModelSelect.value && availableModels.length > 0) {
                        baseModelSelect.value = availableModels[0].id;
                    }
                }

                if (toolModelSelect) {
                    const currentValue = toolModelSelect.value || getSetting('toolModel', '');
                    toolModelSelect.innerHTML = availableModels
                        .map(model => `<option value="${model.id}" ${model.id === currentValue ? 'selected' : ''}>${model.id}</option>`)
                        .join('');
                    if (!toolModelSelect.value && availableModels.length > 0) {
                        toolModelSelect.value = availableModels[0].id;
                    }
                }

                if (visionModelSelect) {
                    const currentValue = visionModelSelect.value || getSetting('visionModel', '');
                    visionModelSelect.innerHTML = availableModels
                        .map(model => `<option value="${model.id}" ${model.id === currentValue ? 'selected' : ''}>${model.id}</option>`)
                        .join('');
                    if (!visionModelSelect.value && availableModels.length > 0) {
                        visionModelSelect.value = availableModels[0].id;
                    }
                }
                
                console.log('Models loaded successfully:', availableModels.length);
            } catch (error) {
                console.error('Error fetching models:', error);
                const errorMsg = `Error: ${error.message || 'Network error'}`;
                if (baseModelSelect) {
                    baseModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                }
                if (toolModelSelect) {
                    toolModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                }
                if (visionModelSelect) {
                    visionModelSelect.innerHTML = `<option value="">${errorMsg}</option>`;
                }
            }
        }

        // Initialize page
        try {
            // Initialize settings on load
            loadToolSettings();
            renderSettings();
            
            // Load Microsoft voices when page loads
            if (speechSynthesis.onvoiceschanged !== undefined) {
                speechSynthesis.onvoiceschanged = loadMicrosoftVoices;
            }
            // Try loading voices immediately (may be empty on first load)
            setTimeout(loadMicrosoftVoices, 100);

            // Re-render settings when drawer opens and fetch models
            if (settingsBtn) {
                settingsBtn.addEventListener('click', () => {
                    try {
                        renderSettings();
                        // Fetch models after a short delay to ensure DOM is ready
                        setTimeout(() => {
                            try {
                                fetchAvailableModels();
                            } catch (e) {
                                console.error('Error fetching models:', e);
                            }
                        }, 100);
                    } catch (e) {
                        console.error('Error rendering settings:', e);
                    }
                });
            }

            console.log('Mobile page initialized');
        } catch (error) {
            console.error('Error initializing mobile page:', error);
            // Show error to user
            if (statusIndicator) {
                statusIndicator.textContent = 'Page initialization error. Please refresh.';
                statusIndicator.style.display = 'block';
            }
        }
    </script>
</body>
</html>
