# Research Plan

- [ ] Define scope and variants
- [ ] Specify exact model(s): PuppyPi vs PuppyPi Pro, Raspberry Pi version, default sensors (camera, mic, LiDAR model), and whether the robotic arm is included or an add-on.
- [ ] Clarify “Integrated with Large AI Model (ChatGPT)”: local vs cloud API, required subscriptions/keys, supported languages, latency expectations.
- [ ] List core features to evaluate: assembly/build quality, ROS readiness, AI vision, voice interaction, LiDAR/SLAM, locomotion, robotic arm, battery life, documentation, and support.
- [ ] Collect official documentation and claims
- [ ] Gather product pages, spec sheets, user manuals, quick-start guides, ROS package repositories, and tutorials from Hiwonder.
- [ ] Record claimed performance (fps of vision, battery runtime, payload, gait speed/stability, voice accuracy, mapping capability), supported OS/ROS distros, and example demos.
- [ ] Note software dependencies (OpenCV/YOLO, speech engines, SLAM stacks like gmapping/hector/cartographer) and version requirements.
- [ ] Map and prioritize review sources
- [ ] Retailer reviews: Amazon, AliExpress, Banggood, Hiwonder store; scrape ratings, verified purchase tags, dates.
- [ ] Community and forums: ROS Answers, ROS Discourse, Reddit (r/robotics, r/ROS, r/raspberry_pi, r/roboticsProjects), Hackster/Instructables, STEM educator forums.
- [ ] Video and blogs: YouTube unboxings/builds/long-term reviews, Bilibili; personal blogs and university lab pages.
- [ ] Code and issue trackers: Hiwonder GitHub and forks; issues, PRs, discussion boards.
- [ ] Regional sources: Chinese marketplaces (JD, Taobao) and Q&A (Zhihu); translate summaries to capture additional feedback.
- [ ] Execute a structured search and data collection protocol
- [ ] Use targeted queries (e.g., “Hiwonder PuppyPi review,” “PuppyPi ROS issues,” “PuppyPi ChatGPT latency,” “PuppyPi LiDAR mapping,” “PuppyPi robotic arm”).
- [ ] Set a date range (last 24 months) and tag older content separately.
- [ ] For each item, record metadata: date, source type, user profile (hobbyist/educator/researcher), configuration (Pi model, ROS distro), environment (indoor/lighting), and whether content is sponsored.
- [ ] Extract and code user feedback into comparable themes
- [ ] Setup and documentation: clarity, time-to-first-run, completeness of tutorials.
- [ ] Mechanical and electrical: assembly difficulty, robustness, servo quality, cable management, battery safety/runtime.
- [ ] ROS and software: install friction, compatibility with Ubuntu/ROS versions, stability of nodes, calibration workflows.
- [ ] AI vision: accuracy, fps, model used, lighting sensitivity, compute bottlenecks on Raspberry Pi.
- [ ] Voice interaction: wake-word reliability, STT/TTS accuracy across accents/noise, end-to-end latency, offline vs cloud.
- [ ] ChatGPT integration: setup complexity, API reliability, latency, cost, privacy concerns, value in real tasks.
- [ ] LiDAR/SLAM: sensor model, mapping quality, drift, loop closure, CPU load, integration with navigation stack.
- [ ] Locomotion: gait stability, terrain handling, noise, speed, tuning tools.
- [ ] Robotic arm: mounting, payload/torque, precision, ROS control, combined use with locomotion.
- [ ] Support and ecosystem: response times, firmware/software updates, spare parts availability, community-created fixes.
- [ ] Tag each item sentiment (positive/negative/neutral) and severity/impact; capture direct quotes.
- [ ] Assess credibility and reconcile conflicting reports
- [ ] Flag sponsored content and vendor-provided units; weigh lower than independent long-term use.
- [ ] Cross-check claims with video evidence, logs, or code snippets; verify software/firmware versions.
- [ ] De-duplicate near-identical reviews; identify patterns by configuration (e.g., Pi 4 vs 5, LiDAR A1 vs A2).
- [ ] Perform deep dives on critical features and pain points
- [ ] AI/voice: measure typical round-trip latency for ChatGPT from reports; note alternatives (local LLMs, Vosk/Picovoice/Whisper) and user-reported trade-offs.
- [ ] SLAM: identify which packages users succeeded with, common parameter tweaks, and mapping artifacts; note CPU/memory constraints and recommended mitigations.
- [ ] Locomotion/arm: gather reports on servo overheating, calibration drift, and payload limits; list proven tuning procedures.
- [ ] Document repeatable workarounds and best practices shared by users.
- [ ] Analyze software maintenance and vendor responsiveness
- [ ] Track GitHub activity: commit frequency, issue resolution time, open vs closed issues, release notes.
- [ ] Compile change logs that address known user complaints; note breaking changes or deprecated demos.
- [ ] Evaluate customer support: email/Discord/Forum presence, RMA experiences, shipping times for parts.
- [ ] Compare with alternatives and contextualize value
- [ ] Select comparable platforms (e.g., Petoi Bittle/Nybble, XiaoR GEEK quadrupeds, Unitree Go1/Go2 Edu tiers, other Hiwonder kits) within similar price/complexity bands.
- [ ] Contrast real-world feedback on setup, reliability, learning curve, and capabilities; identify where PuppyPi excels or lags for education vs research vs hobbyist use.
- [ ] Synthesize findings into actionable guidance
- [ ] Summarize key positives, negatives, and deal-breakers by use case; include expected setup time, must-do tweaks, and hidden costs (ChatGPT API, batteries, replacement servos).
- [ ] Provide a decision checklist and a troubleshooting quick-start based on common issues.
- [ ] Highlight gaps in evidence; propose follow-ups (e.g., short survey/interviews in targeted communities, hands-on validation if resources permit).
- [ ] Deliver an annotated bibliography/database with tagged sources and curated exemplar reviews and videos.
